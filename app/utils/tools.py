from typing import Any

import httpx
from langchain.tools import ToolRuntime, tool
from pydantic import BaseModel, Field

from app.core.config import settings

# ==================== HTTP å®¢æˆ·ç«¯è¿æ¥æ±  ====================


class SandboxHttpClient:
    """
    æ²™ç›’ HTTP å®¢æˆ·ç«¯ç®¡ç†å™¨
    - å¤ç”¨ HTTP è¿æ¥ï¼Œå‡å°‘è¿æ¥å»ºç«‹å¼€é”€
    - æ”¯æŒè¿æ¥æ± 
    """

    _client: httpx.AsyncClient | None = None

    @classmethod
    def get_client(cls) -> httpx.AsyncClient:
        """è·å–æˆ–åˆ›å»º HTTP å®¢æˆ·ç«¯"""
        if cls._client is None or cls._client.is_closed:
            cls._client = httpx.AsyncClient(
                base_url=settings.SANDBOX_URL,
                timeout=settings.SANDBOX_TIMEOUT,
                # è¿æ¥æ± é…ç½®
                limits=httpx.Limits(
                    max_keepalive_connections=10,
                    max_connections=20,
                    keepalive_expiry=30.0,
                ),
            )
        return cls._client

    @classmethod
    async def close(cls) -> None:
        """å…³é—­ HTTP å®¢æˆ·ç«¯"""
        if cls._client is not None and not cls._client.is_closed:
            await cls._client.aclose()
            cls._client = None


# å¿«æ·è®¿é—®å‡½æ•°
def get_sandbox_client() -> httpx.AsyncClient:
    """è·å–æ²™ç›’ HTTP å®¢æˆ·ç«¯"""
    return SandboxHttpClient.get_client()


class RawDataContext(BaseModel):
    """åŸå§‹æ•°æ®ä¸Šä¸‹æ–‡ä¿¡æ¯"""

    id: int
    name: str
    raw_type: str  # "database_table" æˆ– "file"

    # æ–‡ä»¶ç±»å‹
    file_type: str | None = None
    object_key: str | None = None
    bucket_name: str | None = None

    # æ•°æ®åº“è¡¨ç±»å‹
    connection_id: int | None = None
    db_type: str | None = None
    host: str | None = None
    port: int | None = None
    database: str | None = None
    username: str | None = None
    password: str | None = None
    schema_name: str | None = None
    table_name: str | None = None


class DataSourceContext(BaseModel):
    """æ•°æ®æºä¸Šä¸‹æ–‡ä¿¡æ¯"""

    id: int
    name: str
    description: str | None = None
    category: str | None = None

    # å…³è”çš„åŸå§‹æ•°æ®
    raw_data_list: list[RawDataContext] = Field(default_factory=list)

    # ç›®æ ‡å­—æ®µå®šä¹‰
    target_fields: list[dict[str, Any]] | None = None


class ChatContext(BaseModel):
    """èŠå¤©ä¸Šä¸‹æ–‡ - åŒ…å«è¿è¡Œæ—¶é…ç½®å’Œæ•°æ®æºä¿¡æ¯"""

    user_id: int
    thread_id: int
    data_source: DataSourceContext | None = None


# ==================== é”™è¯¯å¤„ç†å·¥å…· ====================


def extract_error_for_llm(error_text: str, max_lines: int = 10) -> str:
    """
    ä»é”™è¯¯ä¿¡æ¯ä¸­æå–å¯¹ LLM æœ‰ä»·å€¼çš„å…³é”®è¡Œã€‚
    ä¿ç•™: é”™è¯¯ç±»å‹ã€é”™è¯¯æ¶ˆæ¯ã€Did you mean å»ºè®®ã€KeyError ç­‰ã€‚
    è¿‡æ»¤: å®Œæ•´çš„ traceback å †æ ˆã€‚
    """
    if not error_text:
        return "æœªçŸ¥é”™è¯¯"

    lines = error_text.split("\n")
    key_lines: list[str] = []
    keywords = ("error", "exception", "did you mean", "keyerror", "invalid", "not found", "ä¸å­˜åœ¨")

    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        # è·³è¿‡ traceback çš„å †æ ˆè¡Œï¼ˆFile "...", line ...ï¼‰
        if stripped.startswith("File ") and ", line " in stripped:
            continue
        # è·³è¿‡çº¯ç¼©è¿›çš„ä»£ç è¡Œ
        if line.startswith("    ") and not any(kw in stripped.lower() for kw in keywords):
            continue
        # ä¿ç•™å…³é”®ä¿¡æ¯è¡Œ
        if any(kw in stripped.lower() for kw in keywords) or len(key_lines) < 3:
            key_lines.append(stripped)

    # é™åˆ¶æœ€å¤§è¡Œæ•°
    result = "\n".join(key_lines[:max_lines])
    return result if result else error_text.split("\n")[0]


# ==================== å·¥å…·å®šä¹‰ ====================


@tool(response_format="content_and_artifact")
async def list_local_files(runtime: ToolRuntime) -> tuple[str, dict[str, Any]]:
    """
    åˆ—å‡ºæ²™ç›’ä¸­çš„æ–‡ä»¶ã€‚
    ç”¨äºæŸ¥çœ‹åˆ†æè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ä¸­é—´æ–‡ä»¶ã€å›¾è¡¨ã€æŠ¥å‘Šç­‰ã€‚

    Returns:
        content: æ–‡ä»¶åˆ—è¡¨æ‘˜è¦ï¼ˆç»™ LLMï¼‰
        artifact: å®Œæ•´æ–‡ä»¶åˆ—è¡¨ï¼ˆç»™å‰ç«¯ï¼‰
    """
    runtime.stream_writer("æ­£åœ¨è·å–æ–‡ä»¶åˆ—è¡¨...")
    ctx: ChatContext = runtime.context  # type: ignore[assignment]
    params = {
        "user_id": ctx.user_id,
        "thread_id": ctx.thread_id,
    }
    client = get_sandbox_client()
    response = await client.get("/files", params=params)
    result = response.json()

    files = result.get("files", [])
    if not files:
        return "å½“å‰ä¼šè¯ç›®å½•ä¸ºç©ºï¼Œæš‚æ— æ–‡ä»¶ã€‚", {"type": "file_list", "files": []}

    # ç»™ LLM çš„æ–‡ä»¶åˆ—è¡¨æ‘˜è¦
    content_lines = [f"ä¼šè¯ç›®å½•ä¸­å…±æœ‰ {len(files)} ä¸ªæ–‡ä»¶ï¼š"]
    for f in files[:10]:  # æœ€å¤šæ˜¾ç¤º 10 ä¸ª
        name = f.get("name", "")
        size = f.get("size", 0)
        size_str = f"{size / 1024:.1f}KB" if size >= 1024 else f"{size}B"
        content_lines.append(f"  - {name} ({size_str})")
    if len(files) > 10:
        content_lines.append(f"  ...ç­‰å…± {len(files)} ä¸ªæ–‡ä»¶")

    return "\n".join(content_lines), {"type": "file_list", "files": files}


@tool(response_format="content_and_artifact")
async def quick_analysis(
    runtime: ToolRuntime,
) -> tuple[str, dict[str, Any]]:
    """
    å¿«é€Ÿåˆ†æå½“å‰ä¼šè¯çš„æ•°æ®æºï¼Œè¿”å›æ•°æ®æ¦‚è§ˆã€‚
    æ”¯æŒæ–‡ä»¶ç±»å‹ï¼ˆCSV/Excel/JSON/Parquetï¼‰å’Œæ•°æ®åº“ç±»å‹ï¼ˆMySQL/PostgreSQLï¼‰ã€‚

    å¯¹äºæ–‡ä»¶ç±»å‹ï¼šè¿”å›è¡Œæ•°ã€åˆ—æ•°ã€ç¼ºå¤±å€¼ç»Ÿè®¡ã€æ•°æ®ç±»å‹ã€ç»Ÿè®¡æ‘˜è¦ã€‚
    å¯¹äºæ•°æ®åº“ç±»å‹ï¼šè¿”å›è¡¨åˆ—è¡¨ã€æ¯ä¸ªè¡¨çš„è¡Œæ•°å’Œåˆ—ä¿¡æ¯ã€‚

    Returns:
        content: æ ¼å¼åŒ–çš„åˆ†ææ‘˜è¦ï¼ˆç»™ LLMï¼‰
        artifact: å®Œæ•´åˆ†æç»“æœï¼ˆç»™å‰ç«¯ï¼‰
    """
    runtime.stream_writer("æ­£åœ¨åˆ†ææ•°æ®æº...")
    ctx: ChatContext = runtime.context  # type: ignore[assignment]

    # ä» context è·å–æ•°æ®æº
    ds_ctx = ctx.data_source
    if ds_ctx is None:
        error_msg = "å½“å‰ä¼šè¯æ²¡æœ‰å…³è”æ•°æ®æº"
        return error_msg, {"type": "error", "error": error_msg}

    if not ds_ctx.raw_data_list:
        error_msg = f"æ•°æ®æº {ds_ctx.name} æ²¡æœ‰å…³è”çš„åŸå§‹æ•°æ®"
        return error_msg, {"type": "error", "error": error_msg}

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŸå§‹æ•°æ®è¿›è¡Œåˆ†æï¼ˆç®€åŒ–å¤„ç†ï¼‰
    raw = ds_ctx.raw_data_list[0]

    # æ„å»ºæ•°æ®æºä¿¡æ¯ä¼ é€’ç»™æ²™ç›’
    data_source_info: dict[str, Any] = {}

    if raw.raw_type == "file":
        data_source_info = {
            "source_type": "file",
            "file_type": raw.file_type,
            "object_key": raw.object_key,
            "bucket_name": raw.bucket_name,
        }
    elif raw.raw_type == "database_table":
        data_source_info = {
            "source_type": "database",
            "db_type": raw.db_type,
            "host": raw.host,
            "port": raw.port,
            "database": raw.database,
            "username": raw.username,
            "password": raw.password,
            "schema_name": raw.schema_name,
            "table_name": raw.table_name,
        }

    client = get_sandbox_client()
    response = await client.post(
        "/quick_analysis",
        params={
            "user_id": ctx.user_id,
            "thread_id": ctx.thread_id,
        },
        json={"data_source": data_source_info},
    )
    result = response.json()

    if not result.get("success"):
        error_msg = result.get("error", "åˆ†æå¤±è´¥")
        return f"æ•°æ®æºåˆ†æå¤±è´¥: {error_msg}", {"type": "error", "error": error_msg}

    analysis = result.get("analysis", {})

    # æ„å»ºæ ¼å¼åŒ–çš„åˆ†ææ‘˜è¦ï¼ˆç»™ LLMï¼‰
    content_lines = [f"## æ•°æ®æº: {ds_ctx.name} (ID: {ds_ctx.id})"]
    content_lines.append(f"- åŸå§‹æ•°æ®: {raw.name}")
    content_lines.append(f"- è¡Œæ•°: {analysis.get('row_count', 'N/A')}")
    content_lines.append(f"- åˆ—æ•°: {analysis.get('column_count', 'N/A')}")

    # åˆ—ä¿¡æ¯
    columns = analysis.get("columns", [])
    if columns:
        content_lines.append("\n### åˆ—ä¿¡æ¯:")
        for col in columns[:15]:  # æœ€å¤šæ˜¾ç¤º 15 åˆ—
            col_name = col.get("name", "")
            col_type = col.get("dtype", "")
            null_count = col.get("null_count", 0)
            null_info = f", ç¼ºå¤± {null_count}" if null_count > 0 else ""
            content_lines.append(f"  - {col_name} ({col_type}{null_info})")
        if len(columns) > 15:
            content_lines.append(f"  ...ç­‰å…± {len(columns)} åˆ—")

    # æ•°å€¼ç»Ÿè®¡æ‘˜è¦
    stats = analysis.get("statistics", {})
    if stats:
        content_lines.append("\n### æ•°å€¼ç»Ÿè®¡æ‘˜è¦:")
        for col_name, col_stats in list(stats.items())[:5]:  # æœ€å¤šæ˜¾ç¤º 5 åˆ—
            mean_val = col_stats.get("mean", "N/A")
            min_val = col_stats.get("min", "N/A")
            max_val = col_stats.get("max", "N/A")

            def _fmt_num(value: Any) -> str:
                """å®‰å…¨æ ¼å¼åŒ–ï¼Œé¿å…éæ•°å€¼ç±»å‹å¯¼è‡´æ ¼å¼åŒ–å¼‚å¸¸ã€‚"""
                return f"{value:.2f}" if isinstance(value, (int, float)) else str(value)

            content_lines.append(
                f"  - {col_name}: å‡å€¼={_fmt_num(mean_val)}, èŒƒå›´=[{_fmt_num(min_val)}, {_fmt_num(max_val)}]"
            )

    # artifact åŒ…å«å®Œæ•´åˆ†æç»“æœ
    artifact = {
        "type": "analysis",
        "data_source_name": ds_ctx.name,
        "data_source_id": ds_ctx.id,
        **analysis,
    }

    return "\n".join(content_lines), artifact


@tool(
    response_format="content_and_artifact",
    description="""ä½¿ç”¨ DuckDB SQL æ–¹è¨€æŸ¥è¯¢æ•°æ®ã€‚
**æ•°æ®è®¿é—®æ–¹å¼**ï¼š
1. æ•°æ®æº VIEWï¼šä½¿ç”¨ RawData åç§°ï¼ˆä¼šè¯åˆå§‹åŒ–æ—¶è‡ªåŠ¨åˆ›å»ºï¼‰
   - `SELECT * FROM "pg_orders" LIMIT 10`
   - `SELECT * FROM "sales_csv"`
2. ä¼šè¯ç›®å½•æ–‡ä»¶ï¼šç›´æ¥è¯»å–æœ¬åœ°æ–‡ä»¶
   - CSV: `SELECT * FROM read_csv_auto('file.csv')`
   - Parquet: `SELECT * FROM 'file.parquet'`
   - JSON: `SELECT * FROM read_json_auto('file.json')`

**ç¤ºä¾‹**ï¼š
- æŸ¥è¯¢ VIEWï¼š`SELECT category, SUM(amount) FROM "pg_orders" GROUP BY category`
- è¯»å–ä¸Šæ¬¡ç»“æœï¼š`SELECT * FROM 'sql_result_xxx.parquet' WHERE amount > 1000`

**é‡è¦**ï¼šç»“æœè‡ªåŠ¨ä¿å­˜ä¸º parquet æ–‡ä»¶ï¼ˆresult_fileï¼‰ï¼Œä¾›åç»­å·¥å…·ä½¿ç”¨""",
)
async def execute_sql(
    sql: str,
    runtime: ToolRuntime,
) -> tuple[str, dict[str, Any]]:
    """
    æ‰§è¡Œ DuckDB SQL æŸ¥è¯¢ã€‚
    æŸ¥è¯¢ä¼šè¯ DuckDB ä¸­çš„ VIEWsï¼ˆä»¥ RawData åç§°å‘½åï¼‰ã€‚

    Args:
        sql: DuckDB SQL æŸ¥è¯¢ï¼Œè¡¨åä½¿ç”¨ RawData åç§°

    Returns:
        content: ç»™ LLM çœ‹çš„ç®€çŸ­æè¿°
        artifact: åŒ…å« SQL å’ŒæŸ¥è¯¢ç»“æœçš„å­—å…¸ï¼ˆå‰ç«¯æ¸²æŸ“ç”¨ï¼‰
    """
    runtime.stream_writer("æ­£åœ¨æ‰§è¡Œ SQL æŸ¥è¯¢...")
    ctx: ChatContext = runtime.context  # type: ignore[assignment]

    client = get_sandbox_client()
    response = await client.post(
        "/execute_sql",
        params={
            "user_id": ctx.user_id,
            "thread_id": ctx.thread_id,
        },
        json={"sql": sql},
    )
    result = response.json()

    if result.get("success"):
        row_count = result.get("row_count", 0)
        columns = result.get("columns", [])
        result_file = result.get("result_file", "")
        rows = result.get("rows", [])

        # æ„å»ºç»™ LLM çš„å†…å®¹
        content_lines = [
            "âœ… SQL æŸ¥è¯¢æˆåŠŸ",
            f"- è¿”å› {row_count} è¡Œæ•°æ®",
            f"- ç»“æœå·²ä¿å­˜è‡³: {result_file}",
            f"- åˆ—å: {', '.join(columns[:10])}{'...' if len(columns) > 10 else ''}",
        ]

        # æ˜¾ç¤ºå‰ 10 è¡Œæ•°æ®é¢„è§ˆï¼ˆç»™ LLM å‚è€ƒï¼‰
        if rows:
            content_lines.append("\nğŸ“Š æ•°æ®é¢„è§ˆ (å‰ 10 è¡Œ):")
            preview_rows = rows[:10]
            # æ„å»ºç®€å•çš„è¡¨æ ¼æ ¼å¼
            for i, row in enumerate(preview_rows):
                row_str = " | ".join(str(v)[:20] for v in row)  # æ¯ä¸ªå€¼æœ€å¤š 20 å­—ç¬¦
                content_lines.append(f"  {i + 1}. {row_str}")
            if row_count > 10:
                content_lines.append(f"  ...å…± {row_count} è¡Œï¼Œå®Œæ•´æ•°æ®è¯·åœ¨å‰ç«¯æŸ¥çœ‹")

        # artifact åŒ…å«æ›´å¤šæ•°æ®ï¼ˆç»™å‰ç«¯æ¸²æŸ“ï¼‰
        max_rows_for_frontend = 100
        artifact = {
            "type": "sql",
            "sql": sql,
            "columns": columns,
            "rows": rows[:max_rows_for_frontend],
            "total_rows": row_count,
            "truncated": len(rows) > max_rows_for_frontend,
            "result_file": result_file,
        }
        return "\n".join(content_lines), artifact
    else:
        error_detail = result.get("error", "æœªçŸ¥é”™è¯¯")
        # ç»™ LLM å…³é”®é”™è¯¯ä¿¡æ¯ï¼ˆä¾¿äºåæ€å’Œä¿®æ­£ï¼‰
        error_for_llm = extract_error_for_llm(error_detail)
        content = f"âŒ SQL æ‰§è¡Œå¤±è´¥:\n{error_for_llm}"
        return content, {
            "type": "error",
            "tool": "execute_sql",
            "sql": sql,
            "error_message": error_detail,  # å®Œæ•´é”™è¯¯ä¿¡æ¯ï¼ˆç»™å‰ç«¯è°ƒè¯•ç”¨ï¼‰
        }


@tool(
    response_format="content_and_artifact",
    description="""æ‰§è¡Œ Python ä»£ç è¿›è¡Œæ•°æ®å¤„ç†ã€‚
**æœ€ä½³å®è·µ**ï¼š
å¦‚æœä½ æ­£åœ¨æ¸…æ´—æ•°æ®ä»¥ä¾¿ç»˜å›¾ï¼Œè¯·åŠ¡å¿…å°†æœ€ç»ˆçš„ DataFrame ä¿å­˜ä¸ºæ–‡ä»¶ã€‚
- æ¨èæ ¼å¼ï¼š`df.to_parquet('analysis_result.parquet')`
- è¿™æ ·ä½ å°±å¯ä»¥åœ¨ `generate_chart` å·¥å…·ä¸­é€šè¿‡ `pd.read_parquet('analysis_result.parquet')` å¿«é€Ÿå¤ç”¨æ•°æ®ã€‚""",
)
async def execute_python(
    code: str,
    runtime: ToolRuntime,
) -> tuple[str, dict[str, Any]]:
    """
    åœ¨æ²™ç›’ä¸­æ‰§è¡Œ Python ä»£ç ï¼Œç”¨äºå¤æ‚æ•°æ®å¤„ç†å’Œåˆ†æã€‚
    å¯ä»¥ä½¿ç”¨ pandasã€numpy ç­‰æ•°æ®åˆ†æåº“ã€‚

    Args:
        code: è¦æ‰§è¡Œçš„ Python ä»£ç 

    Returns:
        content: ç»™ LLM çœ‹çš„ç®€çŸ­æè¿°
        artifact: åŒ…å«ä»£ç å’Œæ‰§è¡Œç»“æœçš„å­—å…¸ï¼ˆå‰ç«¯æ¸²æŸ“ç”¨ï¼‰
    """
    runtime.stream_writer("æ­£åœ¨æ‰§è¡Œ Python ä»£ç ...")
    ctx: ChatContext = runtime.context  # type: ignore[assignment]

    client = get_sandbox_client()
    response = await client.post(
        "/execute_python",
        params={
            "user_id": ctx.user_id,
            "thread_id": ctx.thread_id,
        },
        json={"code": code},
    )
    result = response.json()

    if result.get("success"):
        output = result.get("output", "")
        files_created = result.get("files_created", [])

        # æ„å»ºç»™ LLM çš„å†…å®¹
        content_lines = ["âœ… Python ä»£ç æ‰§è¡ŒæˆåŠŸ"]

        if output:
            # æ˜¾ç¤ºè¾“å‡ºé¢„è§ˆï¼ˆæœ€å¤š 500 å­—ç¬¦ï¼‰
            output_preview = output[:500]
            content_lines.append(f"\nğŸ“ è¾“å‡º:\n{output_preview}")
            if len(output) > 500:
                content_lines.append("...(è¾“å‡ºå·²æˆªæ–­ï¼Œå®Œæ•´è¾“å‡ºè¯·åœ¨å‰ç«¯æŸ¥çœ‹)")

        if files_created:
            content_lines.append(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶: {', '.join(files_created)}")

        artifact = {
            "type": "code",
            "code": code,
            "output": output,
            "files_created": files_created,
        }
        return "\n".join(content_lines), artifact
    else:
        error_detail = result.get("error", "æœªçŸ¥é”™è¯¯")
        output = result.get("output", "")
        # ç»™ LLM å…³é”®é”™è¯¯ä¿¡æ¯ï¼ˆä¾¿äºåæ€å’Œä¿®æ­£ï¼‰
        error_for_llm = extract_error_for_llm(error_detail)
        content = f"âŒ Python æ‰§è¡Œå¤±è´¥:\n{error_for_llm}"
        return content, {
            "type": "error",
            "tool": "execute_python",
            "code": code,
            "output": output,  # æ‰§è¡Œæ—¶çš„æ ‡å‡†è¾“å‡º
            "error_message": error_detail,  # å®Œæ•´é”™è¯¯ä¿¡æ¯ï¼ˆç»™å‰ç«¯è°ƒè¯•ç”¨ï¼‰
        }


@tool(
    response_format="content_and_artifact",
    description="""ä½¿ç”¨ Python Plotly ç»˜åˆ¶å›¾è¡¨ã€‚

**å…³é”®ç­–ç•¥ - æ•°æ®å¤ç”¨**ï¼š
ç›´æ¥è¯»å– execute_sql è‡ªåŠ¨ä¿å­˜çš„ç»“æœæ–‡ä»¶ï¼ˆresult_file å­—æ®µä¸­çš„æ–‡ä»¶åï¼‰ã€‚

**ä»£ç ç¼–å†™è§„èŒƒ**ï¼š
1. **åŠ è½½æ•°æ®**ï¼šä½¿ç”¨ `pd.read_parquet('sql_result_xxx.parquet')` è¯»å– SQL ç»“æœæ–‡ä»¶
2. **å®šä¹‰å¯¹è±¡**ï¼šå¿…é¡»åˆ›å»ºä¸€ä¸ªåä¸º `fig` çš„ Plotly Figure å¯¹è±¡
3. **ç¦æ­¢æ˜¾ç¤º**ï¼šä¸è¦è°ƒç”¨ `fig.show()`

**ç¤ºä¾‹**ï¼š
```python
import pandas as pd
import plotly.express as px
df = pd.read_parquet('sql_result_1234567890.parquet')  # ä½¿ç”¨ execute_sql è¿”å›çš„ result_file
fig = px.bar(df, x='category', y='total_sales', title='é”€å”®é¢åˆ†å¸ƒ')
```""",
)
async def generate_chart(
    code: str,
    title: str,
    runtime: ToolRuntime,
) -> tuple[str, dict[str, Any]]:
    """
    ç”Ÿæˆ Plotly å›¾è¡¨ã€‚
    ä½¿ç”¨ Plotly ç”Ÿæˆå›¾è¡¨ï¼Œä»£ç éœ€è¦åˆ›å»ºåä¸º 'fig' çš„ Plotly figure å¯¹è±¡ã€‚

    Args:
        code: ä½¿ç”¨ Plotly ç”Ÿæˆå›¾è¡¨çš„ Python ä»£ç ï¼Œå¿…é¡»åˆ›å»º fig å˜é‡
        title: å›¾è¡¨æ ‡é¢˜ï¼Œç”¨äºåœ¨å‰ç«¯æ˜¾ç¤º

    Returns:
        content: ç»™ LLM çœ‹çš„ç®€çŸ­æè¿°
        artifact: åŒ…å«å®Œæ•´å›¾è¡¨æ•°æ®çš„å­—å…¸ï¼ˆä¸å‘é€ç»™ LLMï¼‰
    """
    runtime.stream_writer(f"æ­£åœ¨ç”Ÿæˆå›¾è¡¨: {title}...")
    ctx: ChatContext = runtime.context  # type: ignore[assignment]

    client = get_sandbox_client()
    response = await client.post(
        "/generate_chart",
        params={
            "user_id": ctx.user_id,
            "thread_id": ctx.thread_id,
        },
        json={
            "code": code,
        },
    )
    result = response.json()

    if result.get("success"):
        # content: ç»™ LLM çš„ç®€çŸ­æè¿°
        content_lines = [
            f"âœ… å›¾è¡¨ã€Œ{title}ã€ç”ŸæˆæˆåŠŸ",
            "ğŸ“Š å›¾è¡¨æ•°æ®å·²å‘é€è‡³å‰ç«¯æ¸²æŸ“",
            "ğŸ’¡ ç”¨æˆ·å¯ä»¥åœ¨èŠå¤©ç•Œé¢ç›´æ¥æŸ¥çœ‹äº¤äº’å¼ Plotly å›¾è¡¨",
        ]

        # artifact: å®Œæ•´å›¾è¡¨æ•°æ®ï¼ˆç»™å‰ç«¯æ¸²æŸ“ï¼‰
        artifact = {
            "type": "plotly",
            "title": title,
            "chart_json": result.get("chart_json"),  # Plotly JSON æ•°æ®
        }
        return "\n".join(content_lines), artifact
    else:
        error_detail = result.get("error", "æœªçŸ¥é”™è¯¯")
        output = result.get("output", "")
        # ç»™ LLM å…³é”®é”™è¯¯ä¿¡æ¯ï¼ˆä¾¿äºåæ€å’Œä¿®æ­£ï¼‰
        error_for_llm = extract_error_for_llm(error_detail)
        content = f"âŒ å›¾è¡¨ã€Œ{title}ã€ç”Ÿæˆå¤±è´¥:\n{error_for_llm}"
        return content, {
            "type": "error",
            "tool": "generate_chart",
            "title": title,
            "code": code,
            "output": output,
            "error_message": error_detail,  # å®Œæ•´é”™è¯¯ä¿¡æ¯ï¼ˆç»™å‰ç«¯è°ƒè¯•ç”¨ï¼‰
        }
