# Copyright 2025 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import subprocess
import os
import io
import sys
import logging
import traceback
from contextlib import redirect_stdout, redirect_stderr
from pathlib import Path
from typing import Any

from fastapi import FastAPI, UploadFile, File, Query, HTTPException
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, Field

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== å¸¸é‡å®šä¹‰ ====================

SANDBOX_ROOT = Path("/app")

# MinIO é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "localhost:9000")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "admin")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "admin123")
MINIO_SECURE = os.getenv("MINIO_SECURE", "false").lower() == "true"


# ==================== DuckDB è¿æ¥ç®¡ç†å™¨ ====================


class DuckDBConnectionManager:
    """
    DuckDB è¿æ¥ç®¡ç†å™¨
    - å¯åŠ¨æ—¶é¢„åŠ è½½ httpfs æ‰©å±•
    - æä¾›é…ç½®å¥½ S3 è®¿é—®çš„è¿æ¥
    """

    def __init__(self):
        self._extensions_loaded = False
        self._extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        self._extensions_dir.mkdir(parents=True, exist_ok=True)

    def preload_extensions(self) -> None:
        """é¢„åŠ è½½ DuckDB æ‰©å±•ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
        if self._extensions_loaded:
            return

        import duckdb

        logger.info("é¢„åŠ è½½ DuckDB æ‰©å±•...")
        try:
            conn = duckdb.connect(":memory:")
            conn.execute(f"SET extension_directory='{self._extensions_dir}';")
            # é¢„å®‰è£…å¸¸ç”¨æ‰©å±•
            conn.execute("INSTALL httpfs;")
            conn.close()
            self._extensions_loaded = True
            logger.info("DuckDB æ‰©å±•é¢„åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.warning(f"é¢„åŠ è½½ DuckDB æ‰©å±•å¤±è´¥: {e}")

    def get_connection(self, with_s3: bool = False):
        """
        è·å–é…ç½®å¥½çš„ DuckDB è¿æ¥

        Args:
            with_s3: æ˜¯å¦é…ç½® S3 è®¿é—®

        Returns:
            é…ç½®å¥½çš„ DuckDB è¿æ¥
        """
        import duckdb

        conn = duckdb.connect(":memory:")
        conn.execute(f"SET extension_directory='{self._extensions_dir}';")

        if with_s3:
            configure_s3_access(conn)

        return conn


# å…¨å±€è¿æ¥ç®¡ç†å™¨å®ä¾‹
duckdb_manager = DuckDBConnectionManager()


def configure_s3_access(conn) -> None:
    """
    é…ç½® DuckDB è¿æ¥çš„ S3 (MinIO) è®¿é—®ã€‚
    
    åœ¨å·²æœ‰è¿æ¥ä¸Šé…ç½® httpfs æ‰©å±•å’Œ S3 è®¤è¯ä¿¡æ¯ã€‚
    é€‚ç”¨äº session.duckdb æŒä¹…è¿æ¥æˆ–éœ€è¦è®¿é—® S3 çš„åœºæ™¯ã€‚
    
    Args:
        conn: DuckDB è¿æ¥å®ä¾‹
    """
    conn.execute("LOAD httpfs;")
    conn.execute(f"SET s3_endpoint='{MINIO_ENDPOINT}';")
    conn.execute(f"SET s3_access_key_id='{MINIO_ACCESS_KEY}';")
    conn.execute(f"SET s3_secret_access_key='{MINIO_SECRET_KEY}';")
    conn.execute("SET s3_url_style='path';")
    conn.execute(f"SET s3_use_ssl={'true' if MINIO_SECURE else 'false'};")


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================


class ExecuteRequest(BaseModel):
    """Request model for the /execute endpoint."""

    command: str


class ExecuteResponse(BaseModel):
    """Response model for the /execute endpoint."""

    stdout: str
    stderr: str
    exit_code: int


class CodeRequest(BaseModel):
    """Request model for Python code execution."""

    code: str


class SqlRequest(BaseModel):
    """Request model for SQL execution."""

    sql: str
    max_rows: int = Field(default=10000, ge=1, le=100000, description="ç»“æœé›†æœ€å¤§è¡Œæ•°é™åˆ¶")


class ChartRequest(BaseModel):
    """Request model for chart generation."""

    code: str


class DataSourceInfo(BaseModel):
    """æ•°æ®æºä¿¡æ¯æ¨¡å‹"""

    source_type: str  # "file" æˆ– "database"

    # æ–‡ä»¶ç±»å‹æ•°æ®æº (source_type="file")
    file_type: str | None = None  # csv, excel, json, parquet
    object_key: str | None = None  # MinIO å¯¹è±¡ key
    bucket_name: str | None = None  # MinIO bucket åç§°

    # æ•°æ®åº“ç±»å‹æ•°æ®æº (source_type="database")
    db_type: str | None = None  # mysql, postgresql
    host: str | None = None
    port: int | None = None
    database: str | None = None
    username: str | None = None
    password: str | None = None


class QuickAnalysisRequest(BaseModel):
    """å¿«é€Ÿåˆ†æè¯·æ±‚æ¨¡å‹ï¼ˆæ–°ç‰ˆï¼šæ”¯æŒ VIEW å’Œæ–‡ä»¶ï¼‰"""

    # è¦åˆ†æçš„ VIEW åç§°åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™åˆ†ææ‰€æœ‰ VIEW
    view_names: list[str] | None = None
    # è¦åˆ†æçš„ä¼šè¯æ–‡ä»¶åï¼ˆå¦‚ 'sql_result_abcd.parquet'ï¼‰
    file_name: str | None = None



class CodeExecutionResult(BaseModel):
    """Response model for code execution."""

    success: bool
    output: str
    error: str | None = None
    files_created: list[str] = []  # æ‰§è¡Œè¿‡ç¨‹ä¸­åˆ›å»ºçš„æ–‡ä»¶


# ==================== ä¼šè¯åˆå§‹åŒ–æ¨¡å‹ ====================


class RawDataConfig(BaseModel):
    """æ•°æ®å¯¹è±¡é…ç½®"""

    id: str  # UUID å­—ç¬¦ä¸²
    name: str  # ç”¨äºåˆ›å»º VIEW çš„åç§°
    raw_type: str  # "database_table" æˆ– "file"

    # æ•°æ®åº“è¡¨ç±»å‹é…ç½®
    db_type: str | None = None  # mysql, postgresql
    host: str | None = None
    port: int | None = None
    database: str | None = None
    username: str | None = None
    password: str | None = None
    schema_name: str | None = None
    table_name: str | None = None
    custom_sql: str | None = None

    # æ–‡ä»¶ç±»å‹é…ç½®
    file_type: str | None = None  # csv, excel, json, parquet
    object_key: str | None = None
    bucket_name: str | None = None


class FieldMapping(BaseModel):
    """å•ä¸ª RawData çš„å­—æ®µæ˜ å°„é…ç½®"""

    raw_data_id: str  # UUID å­—ç¬¦ä¸²
    raw_data_name: str
    # å­—æ®µæ˜ å°„ï¼š{target_field: source_field}
    mappings: dict[str, str]


class DataSourceConfig(BaseModel):
    """æ•°æ®æºé…ç½®"""

    id: str  # UUID å­—ç¬¦ä¸²
    name: str
    raw_data_list: list[RawDataConfig]
    # ç›®æ ‡å­—æ®µåˆ—è¡¨ï¼ˆç»Ÿä¸€åçš„é€»è¾‘å­—æ®µï¼‰
    target_fields: list[dict] | None = None  # [{name, data_type, description}]
    # å­—æ®µæ˜ å°„é…ç½®åˆ—è¡¨
    raw_mappings: list[FieldMapping] | None = None


class InitSessionRequest(BaseModel):
    """åˆå§‹åŒ–ä¼šè¯è¯·æ±‚"""

    data_source: DataSourceConfig | None = None


# ==================== è¾…åŠ©å‡½æ•° ====================


def get_session_dir(user_id: str, thread_id: str) -> Path:
    """
    è·å–ä¼šè¯å·¥ä½œç›®å½•ã€‚

    ç›®å½•ç»“æ„: /app/sessions/{user_id}/{thread_id}/
    """
    session_dir = SANDBOX_ROOT / "sessions" / str(user_id) / str(thread_id)
    return session_dir


def ensure_session_dir(user_id: str, thread_id: str) -> Path:
    """
    ç¡®ä¿ä¼šè¯ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚

    Returns:
        ä¼šè¯ç›®å½•è·¯å¾„
    """
    session_dir = get_session_dir(user_id, thread_id)
    session_dir.mkdir(parents=True, exist_ok=True)
    return session_dir


def generate_unique_filename(directory: Path, prefix: str, ext: str) -> str:
    """
    ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆ4 ä¸ªéšæœºå­—æ¯ï¼‰ã€‚
    
    Args:
        directory: ç›®æ ‡ç›®å½•
        prefix: æ–‡ä»¶åå‰ç¼€ï¼ˆå¦‚ "sql_result_"ï¼‰
        ext: æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚ ".parquet"ï¼‰
    
    Returns:
        å”¯ä¸€çš„æ–‡ä»¶åï¼ˆå¦‚ "sql_result_abcd.parquet"ï¼‰
    """
    import random
    import string
    
    for _ in range(100):  # æœ€å¤šå°è¯• 100 æ¬¡
        suffix = ''.join(random.choices(string.ascii_lowercase, k=4))
        filename = f"{prefix}{suffix}{ext}"
        if not (directory / filename).exists():
            return filename
    
    # å¦‚æœ 100 æ¬¡éƒ½å†²çªï¼Œä½¿ç”¨æ—¶é—´æˆ³å…œåº•
    import time
    return f"{prefix}{int(time.time())}{ext}"


def list_files_in_dir(directory: Path) -> list[dict[str, Any]]:
    """
    åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆé€’å½’ï¼‰ã€‚

    Returns:
        æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    files = []
    if not directory.exists():
        return files

    for path in directory.rglob("*"):
        if path.is_file():
            rel_path = path.relative_to(directory)
            stat = path.stat()
            files.append(
                {
                    "name": str(rel_path),
                    "size": stat.st_size,
                    "modified": stat.st_mtime,
                }
            )
    return files


# ==================== åº”ç”¨ç”Ÿå‘½å‘¨æœŸ ====================


from contextlib import asynccontextmanager


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
    
    Startup:
    - é¢„åŠ è½½ DuckDB æ‰©å±•ï¼ˆhttpfsã€postgresã€mysql ç­‰ï¼‰
    - ç¡®ä¿æ‰©å±•ç›®å½•å­˜åœ¨
    
    Shutdown:
    - æ¸…ç†ä¸´æ—¶èµ„æºï¼ˆå¦‚æœ‰ï¼‰
    """
    # ===== Startup =====
    logger.info("ğŸš€ Sandbox Runtime å¯åŠ¨ä¸­...")
    
    # é¢„åŠ è½½ DuckDB æ‰©å±•
    duckdb_manager.preload_extensions()
    
    # ç¡®ä¿ sessions ç›®å½•å­˜åœ¨
    sessions_dir = SANDBOX_ROOT / "sessions"
    sessions_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("âœ… Sandbox Runtime å¯åŠ¨å®Œæˆ")
    
    yield  # åº”ç”¨è¿è¡Œä¸­
    
    # ===== Shutdown =====
    logger.info("ğŸ›‘ Sandbox Runtime å…³é—­ä¸­...")
    # ç›®å‰æ²¡æœ‰éœ€è¦æ¸…ç†çš„èµ„æº
    logger.info("ğŸ‘‹ Sandbox Runtime å·²å…³é—­")


# ==================== FastAPI App ====================


app = FastAPI(
    title="Agentic Sandbox Runtime",
    description="An API server for executing commands and managing files in a secure sandbox.",
    version="2.0.0",
    lifespan=lifespan,
)


# ==================== å¥åº·æ£€æŸ¥ ====================


@app.get("/health", summary="Health Check")
async def health_check():
    """A simple health check endpoint to confirm the server is running."""
    return {"status": "ok", "message": "Sandbox Runtime is active."}


# ==================== ä¼šè¯åˆå§‹åŒ– ====================


@app.get("/list_views", summary="List available VIEWs in session DuckDB")
async def list_views(
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åˆ—å‡ºä¼šè¯ DuckDB ä¸­æ‰€æœ‰å¯ç”¨çš„ VIEWã€‚
    
    è¿”å›æ¯ä¸ª VIEW çš„åç§°ã€åˆ—ä¿¡æ¯å’Œè¡Œæ•°ã€‚
    ç”¨äºè®© AI çŸ¥é“å½“å‰å¯ä»¥æŸ¥è¯¢å“ªäº›æ•°æ®ã€‚
    """
    import duckdb

    session_dir = get_session_dir(user_id, thread_id)
    duckdb_path = session_dir / "session.duckdb"

    if not duckdb_path.exists():
        return {
            "success": True,
            "views": [],
            "message": "Session DuckDB not initialized",
        }

    try:
        conn = duckdb.connect(str(duckdb_path), read_only=True)

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        # é…ç½® S3 è®¿é—®ï¼ˆVIEW å¯èƒ½å¼•ç”¨ S3 URLï¼‰
        configure_s3_access(conn)

        # æŸ¥è¯¢æ‰€æœ‰ VIEW
        views_result = conn.execute(
            "SELECT table_name FROM information_schema.tables WHERE table_type = 'VIEW'"
        ).fetchall()

        views_info = []
        for (view_name,) in views_result:
            try:
                # è·å–åˆ—ä¿¡æ¯
                columns_meta = conn.execute(f'PRAGMA table_info("{view_name}")').fetchall()
                columns = [{"name": col[1], "dtype": col[2]} for col in columns_meta]

                # å°è¯•è·å–è¡Œæ•°ï¼ˆå¯èƒ½å› ä¸ºå¤–éƒ¨è¿æ¥é—®é¢˜å¤±è´¥ï¼‰
                try:
                    row_count = conn.execute(f'SELECT COUNT(*) FROM "{view_name}"').fetchone()[0]
                except Exception:
                    row_count = None  # å¤–éƒ¨æ•°æ®æºå¯èƒ½ä¸å¯è¾¾

                views_info.append({
                    "name": view_name,
                    "columns": columns,
                    "column_count": len(columns),
                    "row_count": row_count,
                })
            except Exception as e:
                logger.warning(f"Failed to get info for view {view_name}: {e}")
                views_info.append({
                    "name": view_name,
                    "error": str(e),
                })

        conn.close()

        return {
            "success": True,
            "views": views_info,
            "view_count": len(views_info),
        }

    except Exception as e:
        logger.exception(f"Failed to list views: {e}")
        return {"success": False, "error": str(e), "views": []}


@app.post("/init_session", summary="Initialize session DuckDB with data source")
async def init_session(
    request: InitSessionRequest,
    user_id: str = Query(..., description="User ID (UUID string)"),
    thread_id: str = Query(..., description="Thread/Session ID (UUID string)"),
):
    """
    åˆå§‹åŒ–ä¼šè¯çš„ DuckDB æ–‡ä»¶ã€‚

    åˆ›å»ºä¸€ä¸ªæŒä¹…åŒ–çš„ DuckDB æ–‡ä»¶ï¼Œå¹¶æ ¹æ®æ•°æ®æºé…ç½®ï¼š
    - å®‰è£…å¹¶åŠ è½½å¿…è¦çš„æ‰©å±• (postgres, mysql, httpfs)
    - ATTACH å¤–éƒ¨æ•°æ®åº“
    - ä¸ºæ¯ä¸ª RawData åˆ›å»º VIEW
    """
    import duckdb

    session_dir = ensure_session_dir(user_id, thread_id)
    duckdb_path = session_dir / "session.duckdb"

    try:
        # åˆ›å»º/æ‰“å¼€ DuckDB æ–‡ä»¶
        conn = duckdb.connect(str(duckdb_path))

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        extensions_dir.mkdir(parents=True, exist_ok=True)
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        views_created: list[str] = []
        errors: list[str] = []

        # å¦‚æœæ²¡æœ‰æ•°æ®æºï¼Œåªåˆ›å»ºç©ºçš„ DuckDB æ–‡ä»¶
        if not request.data_source:
            conn.close()
            return {
                "success": True,
                "message": "Session DuckDB initialized (no data source)",
                "duckdb_path": str(duckdb_path),
                "views_created": [],
                "errors": [],
            }

        ds = request.data_source

        # æ„å»º RawData ID åˆ° name çš„æ˜ å°„
        raw_id_to_name: dict[str, str] = {}

        # Step 1: ä¸ºæ¯ä¸ª RawData åˆ›å»ºåŸå§‹ VIEW
        for raw_data in ds.raw_data_list:
            try:
                view_name = raw_data.name  # ä½¿ç”¨ RawData åç§°ä½œä¸º VIEW åç§°
                raw_id_to_name[raw_data.id] = view_name

                if raw_data.raw_type == "database_table":
                    # æ•°æ®åº“è¡¨ç±»å‹ï¼šATTACH æ•°æ®åº“å¹¶åˆ›å»º VIEW
                    if raw_data.db_type == "postgresql":
                        conn.execute("INSTALL postgres; LOAD postgres;")
                        conn_str = (
                            f"host={raw_data.host} "
                            f"port={raw_data.port} "
                            f"dbname={raw_data.database} "
                            f"user={raw_data.username} "
                            f"password={raw_data.password}"
                        )
                        attach_name = f"pg_{raw_data.id}"
                        conn.execute(f"ATTACH '{conn_str}' AS {attach_name} (TYPE POSTGRES, READ_ONLY);")

                        # æ„å»ºæºè¡¨å
                        if raw_data.custom_sql:
                            # ä½¿ç”¨è‡ªå®šä¹‰ SQL
                            conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS {raw_data.custom_sql}')
                        else:
                            # ä½¿ç”¨ schema.table
                            schema = raw_data.schema_name or "public"
                            table = raw_data.table_name
                            conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM {attach_name}.{schema}.{table}')

                    elif raw_data.db_type == "mysql":
                        conn.execute("INSTALL mysql; LOAD mysql;")
                        conn_str = (
                            f"host={raw_data.host} "
                            f"port={raw_data.port} "
                            f"database={raw_data.database} "
                            f"user={raw_data.username} "
                            f"password={raw_data.password}"
                        )
                        attach_name = f"mysql_{raw_data.id}"
                        conn.execute(f"ATTACH '{conn_str}' AS {attach_name} (TYPE MYSQL, READ_ONLY);")

                        if raw_data.custom_sql:
                            conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS {raw_data.custom_sql}')
                        else:
                            table = raw_data.table_name
                            conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM {attach_name}.{table}')

                    views_created.append(view_name)

                elif raw_data.raw_type == "file":
                    # æ–‡ä»¶ç±»å‹ï¼šé€šè¿‡ S3/httpfs åˆ›å»º VIEW
                    configure_s3_access(conn)

                    s3_url = f"s3://{raw_data.bucket_name}/{raw_data.object_key}"

                    if raw_data.file_type == "csv":
                        conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM read_csv_auto(\'{s3_url}\', header=True)')
                    elif raw_data.file_type == "parquet":
                        conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM parquet_scan(\'{s3_url}\')')
                    elif raw_data.file_type == "json":
                        conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM read_json_auto(\'{s3_url}\')')
                    elif raw_data.file_type == "excel":
                        conn.execute("INSTALL spatial; LOAD spatial;")
                        conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM st_read(\'{s3_url}\')')

                    views_created.append(view_name)

            except Exception as e:
                error_msg = f"Failed to create view for {raw_data.name}: {str(e)}"
                logger.warning(error_msg)
                errors.append(error_msg)

        # Step 2: å¦‚æœæœ‰å­—æ®µæ˜ å°„ï¼Œåˆ›å»º DataSource çº§åˆ«çš„ç»Ÿä¸€ VIEW
        if ds.raw_mappings and ds.target_fields and views_created:
            try:
                ds_view_name = ds.name  # DataSource åç§°ä½œä¸ºç»Ÿä¸€ VIEW åç§°

                # è·å–ç›®æ ‡å­—æ®µååˆ—è¡¨
                target_field_names = [f["name"] for f in ds.target_fields]

                # ä¸ºæ¯ä¸ªæœ‰æ˜ å°„çš„ RawData ç”Ÿæˆ SELECT è¯­å¥
                select_parts: list[str] = []
                for mapping in ds.raw_mappings:
                    raw_view_name = raw_id_to_name.get(mapping.raw_data_id)
                    if not raw_view_name or raw_view_name not in views_created:
                        continue

                    # æ„å»ºå­—æ®µé€‰æ‹©åˆ—è¡¨ï¼štarget_field AS source_field
                    field_selects: list[str] = []
                    for target_field in target_field_names:
                        source_field = mapping.mappings.get(target_field)
                        if source_field:
                            # æœ‰æ˜ å°„ï¼šä½¿ç”¨ source_field AS target_field
                            field_selects.append(f'"{source_field}" AS "{target_field}"')
                        else:
                            # æ— æ˜ å°„ï¼šä½¿ç”¨ NULL
                            field_selects.append(f'NULL AS "{target_field}"')

                    if field_selects:
                        select_sql = f'SELECT {", ".join(field_selects)} FROM "{raw_view_name}"'
                        select_parts.append(select_sql)

                if select_parts:
                    # ä½¿ç”¨ UNION ALL åˆå¹¶å¤šä¸ª RawData çš„æ˜ å°„è§†å›¾
                    union_sql = " UNION ALL ".join(select_parts)
                    conn.execute(f'CREATE OR REPLACE VIEW "{ds_view_name}" AS {union_sql}')
                    views_created.append(ds_view_name)
                    logger.info(f"Created DataSource unified VIEW: {ds_view_name}")

            except Exception as e:
                error_msg = f"Failed to create DataSource unified view: {str(e)}"
                logger.warning(error_msg)
                errors.append(error_msg)

        conn.close()

        logger.info(f"Session DuckDB initialized: user_id={user_id}, thread_id={thread_id}, views={len(views_created)}")

        return {
            "success": True,
            "message": f"Session DuckDB initialized with {len(views_created)} views",
            "duckdb_path": str(duckdb_path),
            "views_created": views_created,
            "errors": errors,
        }

    except Exception as e:
        import traceback

        error_traceback = traceback.format_exc()
        logger.exception(f"Failed to initialize session DuckDB: {e}")
        return {"success": False, "error": f"{str(e)}\n\n{error_traceback}"}


# ==================== é‡ç½®æ“ä½œ ====================


@app.post("/reset/session", summary="Reset session files")
async def reset_session(
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    é‡ç½®æŒ‡å®šä¼šè¯çš„æ–‡ä»¶ã€‚
    åˆ é™¤è¯¥ä¼šè¯ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    """
    import shutil

    session_dir = get_session_dir(user_id, thread_id)

    if not session_dir.exists():
        return {
            "success": True,
            "message": "Session directory does not exist, nothing to clean",
            "deleted_count": 0,
        }

    try:
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        files = list_files_in_dir(session_dir)
        deleted_count = len(files)

        # åˆ é™¤ç›®å½•å†…å®¹
        shutil.rmtree(session_dir)
        session_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Reset session: user_id={user_id}, thread_id={thread_id}, deleted={deleted_count} files")

        return {
            "success": True,
            "message": f"Session reset successfully",
            "deleted_count": deleted_count,
        }
    except Exception as e:
        logger.exception(f"Failed to reset session: {e}")
        return {"success": False, "error": str(e)}


@app.post("/reset/user", summary="Reset all user sessions")
async def reset_user(
    user_id: str = Query(..., description="User ID"),
):
    """
    é‡ç½®æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯æ–‡ä»¶ã€‚
    åˆ é™¤è¯¥ç”¨æˆ·ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    """
    import shutil

    user_dir = SANDBOX_ROOT / "sessions" / str(user_id)

    if not user_dir.exists():
        return {
            "success": True,
            "message": "User directory does not exist, nothing to clean",
            "deleted_count": 0,
            "session_count": 0,
        }

    try:
        # ç»Ÿè®¡ä¼šè¯å’Œæ–‡ä»¶æ•°é‡
        session_count = len([d for d in user_dir.iterdir() if d.is_dir()])
        file_count = 0
        for session_dir in user_dir.iterdir():
            if session_dir.is_dir():
                file_count += len(list_files_in_dir(session_dir))

        # åˆ é™¤ç”¨æˆ·ç›®å½•
        shutil.rmtree(user_dir)

        logger.info(f"Reset user: user_id={user_id}, deleted={file_count} files in {session_count} sessions")

        return {
            "success": True,
            "message": f"User data reset successfully",
            "deleted_count": file_count,
            "session_count": session_count,
        }
    except Exception as e:
        logger.exception(f"Failed to reset user: {e}")
        return {"success": False, "error": str(e)}


@app.post("/reset/all", summary="Reset all sandbox data")
async def reset_all():
    """
    é‡ç½®æ‰€æœ‰æ²™ç›’æ•°æ®ã€‚
    åˆ é™¤ sessions ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    ä»…ç”¨äºç®¡ç†ç›®çš„ï¼Œè°¨æ…ä½¿ç”¨ã€‚
    """
    import shutil

    sessions_dir = SANDBOX_ROOT / "sessions"

    if not sessions_dir.exists():
        return {
            "success": True,
            "message": "Sessions directory does not exist, nothing to clean",
            "deleted_count": 0,
            "user_count": 0,
        }

    try:
        # ç»Ÿè®¡ç”¨æˆ·å’Œæ–‡ä»¶æ•°é‡
        user_count = len([d for d in sessions_dir.iterdir() if d.is_dir()])
        file_count = 0
        for user_dir in sessions_dir.iterdir():
            if user_dir.is_dir():
                for session_dir in user_dir.iterdir():
                    if session_dir.is_dir():
                        file_count += len(list_files_in_dir(session_dir))

        # åˆ é™¤æ•´ä¸ª sessions ç›®å½•å¹¶é‡å»º
        shutil.rmtree(sessions_dir)
        sessions_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Reset all: deleted={file_count} files from {user_count} users")

        return {
            "success": True,
            "message": "All sandbox data reset successfully",
            "deleted_count": file_count,
            "user_count": user_count,
        }
    except Exception as e:
        logger.exception(f"Failed to reset all: {e}")
        return {"success": False, "error": str(e)}


# ==================== æ–‡ä»¶ç®¡ç† ====================


@app.get("/files", summary="List files in session directory")
async def list_files(
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åˆ—å‡ºä¼šè¯ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    ç”¨äºæŸ¥çœ‹åˆ†æè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ä¸­é—´æ–‡ä»¶ã€å›¾è¡¨ã€æŠ¥å‘Šç­‰ã€‚
    """
    session_dir = ensure_session_dir(user_id, thread_id)
    files = list_files_in_dir(session_dir)

    return {
        "success": True,
        "files": files,
        "count": len(files),
    }


@app.post("/upload", summary="Upload a file to the session directory")
async def upload_file(
    file: UploadFile = File(...),
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯ç›®å½•ã€‚
    ç”¨æˆ·æ— éœ€å…³å¿ƒå…·ä½“å­˜å‚¨è·¯å¾„ï¼Œæ–‡ä»¶è‡ªåŠ¨ä¿å­˜åˆ°å¯¹åº”ä¼šè¯ç›®å½•ã€‚
    """
    try:
        session_dir = ensure_session_dir(user_id, thread_id)
        file_path = session_dir / file.filename

        # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨ï¼ˆå¤„ç†å¸¦è·¯å¾„çš„æ–‡ä»¶åï¼‰
        file_path.parent.mkdir(parents=True, exist_ok=True)

        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)

        logger.info(f"File uploaded: {file_path}")

        return JSONResponse(
            status_code=200,
            content={
                "success": True,
                "message": f"File '{file.filename}' uploaded successfully.",
                "path": str(file_path.relative_to(session_dir)),
            },
        )
    except Exception as e:
        logger.exception("File upload failed")
        return JSONResponse(
            status_code=500, content={"success": False, "message": f"Upload failed: {e!s}"}
        )


@app.get("/download/{file_path:path}", summary="Download a file from the session directory")
async def download_file(
    file_path: str,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    ä»ä¼šè¯ç›®å½•ä¸‹è½½æ–‡ä»¶ã€‚
    file_path æ˜¯ç›¸å¯¹äºä¼šè¯ç›®å½•çš„è·¯å¾„ã€‚
    """
    session_dir = ensure_session_dir(user_id, thread_id)
    full_path = session_dir / file_path

    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„ç©¿è¶Š
    try:
        full_path.resolve().relative_to(session_dir.resolve())
    except ValueError:
        raise HTTPException(status_code=403, detail="Access denied: path traversal detected")

    if full_path.is_file():
        return FileResponse(
            path=str(full_path), media_type="application/octet-stream", filename=Path(file_path).name
        )

    raise HTTPException(status_code=404, detail="File not found")


# ==================== ä»£ç æ‰§è¡Œ ====================


@app.post("/execute", summary="Execute a shell command", response_model=ExecuteResponse)
async def execute_command(
    request: ExecuteRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åœ¨ä¼šè¯ç›®å½•ä¸­æ‰§è¡Œ Shell å‘½ä»¤ã€‚
    å‘½ä»¤çš„å·¥ä½œç›®å½•è‡ªåŠ¨è®¾ç½®ä¸ºä¼šè¯ç›®å½•ã€‚
    """
    try:
        session_dir = ensure_session_dir(user_id, thread_id)

        # ä½¿ç”¨ shell=True ä»¥æ”¯æŒç®¡é“å’Œé‡å®šå‘
        process = subprocess.run(
            request.command,
            shell=True,
            capture_output=True,
            text=True,
            cwd=str(session_dir),
            timeout=60,
        )

        return ExecuteResponse(
            stdout=process.stdout, stderr=process.stderr, exit_code=process.returncode
        )
    except subprocess.TimeoutExpired:
        return ExecuteResponse(stdout="", stderr="Command execution timeout (60s)", exit_code=124)
    except Exception as e:
        return ExecuteResponse(stdout="", stderr=f"Failed to execute command: {e!s}", exit_code=1)


@app.post("/execute_python", summary="Execute Python code")
async def execute_python(
    request: CodeRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åœ¨æ²™ç›’ä¸­æ‰§è¡Œ Python ä»£ç ã€‚
    ä»£ç å¯ä»¥è®¿é—® pandasã€numpy ç­‰æ•°æ®åˆ†æåº“ã€‚
    ç”Ÿæˆçš„æ–‡ä»¶ä¼šä¿å­˜åˆ°ä¼šè¯ç›®å½•ã€‚
    """
    session_dir = ensure_session_dir(user_id, thread_id)

    # è·å–æ‰§è¡Œå‰çš„æ–‡ä»¶åˆ—è¡¨
    files_before = set(f["name"] for f in list_files_in_dir(session_dir))

    # æ•è·è¾“å‡º
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()

    # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
    exec_globals = {
        "__builtins__": __builtins__,
        "__name__": "__main__",
        "WORK_DIR": session_dir,
    }

    # åˆ‡æ¢åˆ°ä¼šè¯ç›®å½•
    original_cwd = os.getcwd()
    original_path = sys.path.copy()

    try:
        os.chdir(session_dir)
        sys.path.insert(0, str(session_dir))

        with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):
            exec(request.code, exec_globals)

        # è·å–æ–°åˆ›å»ºçš„æ–‡ä»¶
        files_after = set(f["name"] for f in list_files_in_dir(session_dir))
        files_created = list(files_after - files_before)

        return CodeExecutionResult(
            success=True,
            output=stdout_buffer.getvalue(),
            files_created=files_created,
        )

    except Exception as e:
        error_traceback = traceback.format_exc()
        return CodeExecutionResult(
            success=False,
            output=stdout_buffer.getvalue(),
            error=f"{e!s}\n\n{error_traceback}",
        )
    finally:
        os.chdir(original_cwd)
        sys.path = original_path


@app.post("/execute_sql", summary="Execute SQL query using DuckDB")
async def execute_sql(
    request: SqlRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    ä½¿ç”¨ä¼šè¯çš„ DuckDB æ–‡ä»¶æ‰§è¡Œ SQL æŸ¥è¯¢ã€‚

    æ•°æ®é€šè¿‡ä¼šè¯åˆå§‹åŒ–æ—¶åˆ›å»ºçš„ VIEWs è®¿é—®ï¼ŒAI å¯ä»¥ç›´æ¥æŸ¥è¯¢è¿™äº› VIEWsã€‚
    """
    import duckdb

    session_dir = ensure_session_dir(user_id, thread_id)
    duckdb_path = session_dir / "session.duckdb"

    try:
        # å¦‚æœä¼šè¯ DuckDB æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„
        if not duckdb_path.exists():
            logger.warning(f"Session DuckDB not found, creating empty: {duckdb_path}")

        # æ‰“å¼€ä¼šè¯çš„ DuckDB æ–‡ä»¶
        conn = duckdb.connect(str(duckdb_path))

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        # é…ç½® S3 è®¿é—®ï¼ˆç”¨äºè¯»å–ä¼šè¯ç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶ï¼‰
        configure_s3_access(conn)

        # åˆ‡æ¢å·¥ä½œç›®å½•ä»¥ä¾¿ç›¸å¯¹è·¯å¾„è®¿é—®æœ¬åœ°æ–‡ä»¶
        original_cwd = os.getcwd()
        os.chdir(session_dir)

        try:
            # å…ˆç”¨ EXPLAIN æ£€æŸ¥ SQL è¯­æ³•ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰
            try:
                conn.execute(f"EXPLAIN {request.sql}")
            except Exception as explain_error:
                # è¯­æ³•é”™è¯¯ï¼Œç›´æ¥è¿”å›é”™è¯¯ä¿¡æ¯
                error_msg = str(explain_error)
                logger.warning(f"SQL syntax check failed: {error_msg}")
                return {"success": False, "error": error_msg}

            # è¯­æ³•æ£€æŸ¥é€šè¿‡ï¼Œæ‰§è¡Œå®é™…æŸ¥è¯¢
            result = conn.execute(request.sql)
            columns = [desc[0] for desc in result.description] if result.description else []

            # ä½¿ç”¨ fetchmany é™åˆ¶å†…å­˜ä½¿ç”¨ï¼Œé¿å…å¤§ç»“æœé›†å¯¼è‡´ OOM
            max_rows = request.max_rows
            rows = result.fetchmany(max_rows + 1)  # å¤šå–ä¸€è¡Œç”¨äºæ£€æµ‹æ˜¯å¦æœ‰æ›´å¤šæ•°æ®
            has_more = len(rows) > max_rows
            if has_more:
                rows = rows[:max_rows]  # æˆªæ–­åˆ°é™åˆ¶è¡Œæ•°
                logger.warning(f"SQL result truncated to {max_rows} rows (has more data)")

            # è‡ªåŠ¨ä¿å­˜ç»“æœåˆ° parquet æ–‡ä»¶ï¼ˆä¾›åç»­å·¥å…·ä½¿ç”¨ï¼‰
            result_file = None
            if rows and columns:
                import pandas as pd

                try:
                    df = pd.DataFrame(rows, columns=columns)
                    result_file = generate_unique_filename(session_dir, "sql_result_", ".parquet")
                    df.to_parquet(session_dir / result_file, index=False)
                    logger.info(f"SQL result saved to {result_file}")
                except Exception as e:
                    logger.warning(f"Failed to save SQL result: {e}")

            return {
                "success": True,
                "columns": columns,
                "rows": [list(row) for row in rows],
                "row_count": len(rows),
                "result_file": result_file,  # ç»“æœæ–‡ä»¶è·¯å¾„
                "truncated": has_more,  # æ˜¯å¦è¢«æˆªæ–­
                "max_rows": max_rows,  # æœ€å¤§è¡Œæ•°é™åˆ¶
            }
        finally:
            os.chdir(original_cwd)
            conn.close()

    except Exception as e:
        import traceback

        error_traceback = traceback.format_exc()
        logger.exception("SQL execution failed")
        return {"success": False, "error": f"{e!s}\n\n{error_traceback}"}


# ==================== æ•°æ®åˆ†æ ====================


def setup_duckdb_s3(conn) -> None:
    """
    é…ç½® DuckDB ä»¥è®¿é—® MinIO (S3 å…¼å®¹)ã€‚
    
    åŒ…å« INSTALL httpfsï¼ˆç”¨äºé¦–æ¬¡æœªé¢„åŠ è½½çš„åœºæ™¯ï¼‰ã€‚
    å¦‚æœæ‰©å±•å·²é¢„åŠ è½½ï¼Œä½¿ç”¨ configure_s3_access() å³å¯ã€‚
    """
    # è®¾ç½®æ‰©å±•ç›®å½•åˆ°å¯å†™è·¯å¾„
    extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
    extensions_dir.mkdir(parents=True, exist_ok=True)
    conn.execute(f"SET extension_directory='{extensions_dir}';")
    
    conn.execute("INSTALL httpfs;")
    configure_s3_access(conn)


def get_db_connection_string(ds: DataSourceInfo) -> str:
    """æ„å»ºæ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²"""
    if ds.db_type == "postgresql":
        return f"postgresql://{ds.username}:{ds.password}@{ds.host}:{ds.port}/{ds.database}"
    elif ds.db_type == "mysql":
        return f"mysql://{ds.username}:{ds.password}@{ds.host}:{ds.port}/{ds.database}"
    else:
        raise ValueError(f"Unsupported database type: {ds.db_type}")


def analyze_data_with_duckdb(conn, table_or_view: str = "data_preview") -> dict[str, Any]:
    """ä½¿ç”¨ DuckDB åˆ†ææ•°æ®ï¼Œè¿”å›ç»Ÿè®¡ç»“æœ"""
    row_count = conn.execute(f"SELECT COUNT(*) FROM {table_or_view}").fetchone()[0]
    columns_meta = conn.execute(f"PRAGMA table_info('{table_or_view}')").fetchall()

    numeric_types = {
        "TINYINT", "SMALLINT", "INTEGER", "BIGINT", "HUGEINT",
        "REAL", "DOUBLE", "FLOAT", "DECIMAL", "NUMERIC",
    }

    analysis_columns = []
    missing_values = {}

    for _, col_name, col_type, *_ in columns_meta:
        # ç¼ºå¤±å€¼ç»Ÿè®¡
        null_count = conn.execute(
            f'SELECT COUNT(*) FROM {table_or_view} WHERE "{col_name}" IS NULL'
        ).fetchone()[0]

        col_info: dict[str, Any] = {
            "name": col_name,
            "dtype": col_type,
            "non_null_count": int(row_count - null_count),
            "null_count": int(null_count),
        }

        # æ•°å€¼åˆ—ç»Ÿè®¡
        if col_type.upper() in numeric_types and (row_count - null_count) > 0:
            stats_row = conn.execute(
                f'''
                SELECT
                    AVG(CAST("{col_name}" AS DOUBLE)) AS mean,
                    STDDEV_POP(CAST("{col_name}" AS DOUBLE)) AS std,
                    MIN(CAST("{col_name}" AS DOUBLE)) AS min,
                    MAX(CAST("{col_name}" AS DOUBLE)) AS max,
                    MEDIAN(CAST("{col_name}" AS DOUBLE)) AS median
                FROM {table_or_view}
                WHERE "{col_name}" IS NOT NULL
                '''
            ).fetchone()

            col_info["stats"] = {
                "mean": float(stats_row[0]) if stats_row[0] is not None else None,
                "std": float(stats_row[1]) if stats_row[1] is not None else None,
                "min": float(stats_row[2]) if stats_row[2] is not None else None,
                "max": float(stats_row[3]) if stats_row[3] is not None else None,
                "median": float(stats_row[4]) if stats_row[4] is not None else None,
            }

        analysis_columns.append(col_info)
        missing_values[col_name] = int(null_count)

    return {
        "row_count": int(row_count),
        "column_count": len(columns_meta),
        "columns": analysis_columns,
        "missing_values": missing_values,
    }


def setup_duckdb_extensions_dir(conn) -> None:
    """è®¾ç½® DuckDB æ‰©å±•ç›®å½•åˆ°å¯å†™è·¯å¾„"""
    extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
    extensions_dir.mkdir(parents=True, exist_ok=True)
    conn.execute(f"SET extension_directory='{extensions_dir}';")


@app.post("/quick_analysis", summary="Quick data analysis")
async def quick_analysis(
    request: QuickAnalysisRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    å¿«é€Ÿåˆ†ææ•°æ®ï¼Œæ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    
    1. åˆ†æä¼šè¯æ–‡ä»¶ï¼šæŒ‡å®š file_name å‚æ•°
    2. åˆ†ææ•°æ®æº VIEWï¼šæŒ‡å®š view_names æˆ–ç•™ç©ºåˆ†ææ‰€æœ‰ VIEW
    """
    import duckdb
    import os

    session_dir = get_session_dir(user_id, thread_id)

    # ===== æ¨¡å¼ 1ï¼šåˆ†æä¼šè¯æ–‡ä»¶ =====
    if request.file_name:
        file_path = session_dir / request.file_name

        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„ç©¿è¶Š
        try:
            file_path.resolve().relative_to(session_dir.resolve())
        except ValueError:
            return {"success": False, "error": "Invalid file path: path traversal detected"}

        if not file_path.exists():
            return {"success": False, "error": f"File not found: {request.file_name}"}

        conn = None
        try:
            conn = duckdb.connect(":memory:")
            extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
            conn.execute(f"SET extension_directory='{extensions_dir}';")

            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–æ–¹å¼
            file_ext = file_path.suffix.lower()
            original_cwd = os.getcwd()
            os.chdir(session_dir)

            try:
                if file_ext == ".parquet":
                    conn.execute(f"CREATE OR REPLACE TEMP VIEW data_preview AS SELECT * FROM '{request.file_name}'")
                elif file_ext == ".csv":
                    conn.execute(f"CREATE OR REPLACE TEMP VIEW data_preview AS SELECT * FROM read_csv_auto('{request.file_name}', header=True)")
                elif file_ext == ".json":
                    conn.execute(f"CREATE OR REPLACE TEMP VIEW data_preview AS SELECT * FROM read_json_auto('{request.file_name}')")
                else:
                    return {"success": False, "error": f"Unsupported file type: {file_ext}"}

                analysis = analyze_data_with_duckdb(conn, "data_preview")
                analysis["file_name"] = request.file_name

                return {"success": True, "analysis": analysis}
            finally:
                os.chdir(original_cwd)

        except Exception as e:
            logger.exception(f"Failed to analyze file {request.file_name}")
            return {"success": False, "error": str(e)}
        finally:
            if conn:
                conn.close()

    # ===== æ¨¡å¼ 2ï¼šåˆ†ææ•°æ®æº VIEW =====
    duckdb_path = session_dir / "session.duckdb"

    if not duckdb_path.exists():
        return {
            "success": False,
            "error": "Session DuckDB not initialized. Please create a session with data source first.",
        }

    conn = None
    try:
        conn = duckdb.connect(str(duckdb_path), read_only=True)

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        # é…ç½® S3 è®¿é—®ï¼ˆVIEW å¯èƒ½å¼•ç”¨ S3 URLï¼‰
        configure_s3_access(conn)

        # è·å–è¦åˆ†æçš„ VIEW åˆ—è¡¨
        if request.view_names:
            view_names = request.view_names
        else:
            # æŸ¥è¯¢æ‰€æœ‰ VIEW
            views_result = conn.execute(
                "SELECT table_name FROM information_schema.tables WHERE table_type = 'VIEW'"
            ).fetchall()
            view_names = [row[0] for row in views_result]

        if not view_names:
            return {
                "success": True,
                "analysis": {
                    "views": [],
                    "message": "No VIEWs found in session DuckDB",
                },
            }

        # åˆ†ææ¯ä¸ª VIEW
        views_analysis = []
        for view_name in view_names:
            try:
                analysis = analyze_data_with_duckdb(conn, f'"{view_name}"')
                analysis["view_name"] = view_name
                views_analysis.append(analysis)
            except Exception as e:
                logger.warning(f"Failed to analyze view {view_name}: {e}")
                views_analysis.append({
                    "view_name": view_name,
                    "error": str(e),
                })

        # å¦‚æœåªæœ‰ä¸€ä¸ª VIEWï¼Œç®€åŒ–è¿”å›ç»“æ„
        if len(views_analysis) == 1:
            result_analysis = views_analysis[0]
        else:
            result_analysis = {
                "view_count": len(views_analysis),
                "views": views_analysis,
            }

        return {"success": True, "analysis": result_analysis}

    except Exception as e:
        logger.exception("Quick analysis failed")
        return {"success": False, "error": str(e)}
    finally:
        if conn:
            conn.close()

# ==================== å›¾è¡¨ç”Ÿæˆ ====================


@app.post("/generate_chart", summary="Generate Plotly chart")
async def generate_chart(
    request: ChartRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    æ‰§è¡Œ Python ä»£ç ç”Ÿæˆ Plotly å›¾è¡¨ã€‚

    ä»£ç åº”è¯¥ä½¿ç”¨ plotly åº“åˆ›å»ºå›¾è¡¨ï¼Œå¹¶å°† figure å¯¹è±¡èµ‹å€¼ç»™ `fig` å˜é‡ã€‚
    ç¤ºä¾‹ä»£ç ï¼š
    ```python
    import plotly.express as px
    import pandas as pd

    df = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]})
    fig = px.bar(df, x='x', y='y', title='ç¤ºä¾‹å›¾è¡¨')
    ```
    """
    session_dir = ensure_session_dir(user_id, thread_id)

    # æ•è·è¾“å‡º
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()

    # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
    exec_globals = {
        "__builtins__": __builtins__,
        "__name__": "__main__",
        "WORK_DIR": session_dir,
    }

    # åˆ‡æ¢åˆ°ä¼šè¯ç›®å½•
    original_cwd = os.getcwd()
    original_path = sys.path.copy()

    try:
        os.chdir(session_dir)
        sys.path.insert(0, str(session_dir))

        with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):
            exec(request.code, exec_globals)

        # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº† fig å˜é‡
        fig = exec_globals.get("fig")
        if fig is None:
            return {
                "success": False,
                "error": "ä»£ç æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªæ‰¾åˆ° 'fig' å˜é‡ã€‚è¯·ç¡®ä¿ä»£ç åˆ›å»ºäº†åä¸º 'fig' çš„ Plotly figure å¯¹è±¡ã€‚",
                "output": stdout_buffer.getvalue(),
            }

        # åŒæ—¶ä¿å­˜ä¸º JSON ä»¥ä¾¿å‰ç«¯æ¸²æŸ“
        chart_json = fig.to_json()

        return {
            "success": True,
            "chart_json": chart_json,
            "output": stdout_buffer.getvalue(),
        }

    except Exception as e:
        error_traceback = traceback.format_exc()
        return {
            "success": False,
            "output": stdout_buffer.getvalue(),
            "error": f"{e!s}\n\n{error_traceback}",
        }
    finally:
        os.chdir(original_cwd)
        sys.path = original_path
