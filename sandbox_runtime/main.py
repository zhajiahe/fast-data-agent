# Copyright 2025 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
æ²™ç®±è¿è¡Œæ—¶ä¸»æ¨¡å—

æä¾›å®‰å…¨çš„ä»£ç æ‰§è¡Œã€SQL æŸ¥è¯¢ã€æ–‡ä»¶ç®¡ç†ç­‰ APIã€‚
"""

import io
import logging
import os
import subprocess
import sys
import traceback
from contextlib import asynccontextmanager, redirect_stderr, redirect_stdout
from pathlib import Path

from fastapi import FastAPI, File, HTTPException, Query, UploadFile
from fastapi.responses import FileResponse, JSONResponse

# ä»æ¨¡å—å¯¼å…¥
from sandbox_runtime.models import (
    ChartRequest,
    CodeExecutionResult,
    CodeRequest,
    ExecuteRequest,
    ExecuteResponse,
    InitSessionRequest,
    QuickAnalysisRequest,
    SqlRequest,
)
from sandbox_runtime.services import (
    FileService,
    analyze_data_with_duckdb,
    configure_s3_access,
    duckdb_manager,
)
from sandbox_runtime.utils import (
    SANDBOX_ROOT,
    ensure_session_dir,
    generate_unique_filename,
    get_session_dir,
    list_files_in_dir,
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================== åº”ç”¨ç”Ÿå‘½å‘¨æœŸ ====================


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†

    Startup:
    - é¢„åŠ è½½ DuckDB æ‰©å±•ï¼ˆhttpfsã€postgresã€mysql ç­‰ï¼‰
    - ç¡®ä¿æ‰©å±•ç›®å½•å­˜åœ¨

    Shutdown:
    - æ¸…ç†ä¸´æ—¶èµ„æºï¼ˆå¦‚æœ‰ï¼‰
    """
    # ===== Startup =====
    logger.info("ğŸš€ Sandbox Runtime å¯åŠ¨ä¸­...")

    # é¢„åŠ è½½ DuckDB æ‰©å±•
    duckdb_manager.preload_extensions()

    # ç¡®ä¿ sessions ç›®å½•å­˜åœ¨
    sessions_dir = SANDBOX_ROOT / "sessions"
    sessions_dir.mkdir(parents=True, exist_ok=True)

    logger.info("âœ… Sandbox Runtime å¯åŠ¨å®Œæˆ")

    yield  # åº”ç”¨è¿è¡Œä¸­

    # ===== Shutdown =====
    logger.info("ğŸ›‘ Sandbox Runtime å…³é—­ä¸­...")
    # ç›®å‰æ²¡æœ‰éœ€è¦æ¸…ç†çš„èµ„æº
    logger.info("ğŸ‘‹ Sandbox Runtime å·²å…³é—­")


# ==================== FastAPI App ====================


app = FastAPI(
    title="Agentic Sandbox Runtime",
    description="An API server for executing commands and managing files in a secure sandbox.",
    version="2.0.0",
    lifespan=lifespan,
)


# ==================== å¥åº·æ£€æŸ¥ ====================


@app.get("/health", summary="Health Check")
async def health_check():
    """A simple health check endpoint to confirm the server is running."""
    return {"status": "ok", "message": "Sandbox Runtime is active."}


# ==================== ä¼šè¯åˆå§‹åŒ– ====================


@app.get("/list_views", summary="List available VIEWs in session DuckDB")
async def list_views(
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åˆ—å‡ºä¼šè¯ DuckDB ä¸­æ‰€æœ‰å¯ç”¨çš„ VIEWã€‚

    è¿”å›æ¯ä¸ª VIEW çš„åç§°ã€åˆ—ä¿¡æ¯å’Œè¡Œæ•°ã€‚
    ç”¨äºè®© AI çŸ¥é“å½“å‰å¯ä»¥æŸ¥è¯¢å“ªäº›æ•°æ®ã€‚
    """
    import duckdb

    session_dir = get_session_dir(user_id, thread_id)
    duckdb_path = session_dir / "session.duckdb"

    if not duckdb_path.exists():
        return {
            "success": True,
            "views": [],
            "message": "Session DuckDB not initialized",
        }

    try:
        conn = duckdb.connect(str(duckdb_path), read_only=True)

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        # é…ç½® S3 è®¿é—®ï¼ˆVIEW å¯èƒ½å¼•ç”¨ S3 URLï¼‰
        configure_s3_access(conn)

        # æŸ¥è¯¢æ‰€æœ‰ VIEW
        views_result = conn.execute(
            "SELECT table_name FROM information_schema.tables WHERE table_type = 'VIEW'"
        ).fetchall()

        views_info = []
        for (view_name,) in views_result:
            try:
                # è·å–åˆ—ä¿¡æ¯
                columns_meta = conn.execute(f'PRAGMA table_info("{view_name}")').fetchall()
                columns = [{"name": col[1], "dtype": col[2]} for col in columns_meta]

                # å°è¯•è·å–è¡Œæ•°ï¼ˆå¯èƒ½å› ä¸ºå¤–éƒ¨è¿æ¥é—®é¢˜å¤±è´¥ï¼‰
                try:
                    row_count = conn.execute(f'SELECT COUNT(*) FROM "{view_name}"').fetchone()[0]
                except Exception:
                    row_count = None  # å¤–éƒ¨æ•°æ®æºå¯èƒ½ä¸å¯è¾¾

                views_info.append(
                    {
                        "name": view_name,
                        "columns": columns,
                        "column_count": len(columns),
                        "row_count": row_count,
                    }
                )
            except Exception as e:
                logger.warning(f"Failed to get info for view {view_name}: {e}")
                views_info.append(
                    {
                        "name": view_name,
                        "error": str(e),
                    }
                )

        conn.close()

        return {
            "success": True,
            "views": views_info,
            "view_count": len(views_info),
        }

    except Exception as e:
        logger.exception(f"Failed to list views: {e}")
        return {"success": False, "error": str(e), "views": []}


@app.post("/init_session", summary="Initialize session DuckDB with raw data")
async def init_session(
    request: InitSessionRequest,
    user_id: str = Query(..., description="User ID (UUID string)"),
    thread_id: str = Query(..., description="Thread/Session ID (UUID string)"),
):
    """
    åˆå§‹åŒ–ä¼šè¯çš„ DuckDB æ–‡ä»¶ã€‚

    åˆ›å»ºä¸€ä¸ªæŒä¹…åŒ–çš„ DuckDB æ–‡ä»¶ï¼Œå¹¶æ ¹æ®æ•°æ®å¯¹è±¡é…ç½®ï¼š
    - å®‰è£…å¹¶åŠ è½½å¿…è¦çš„æ‰©å±• (postgres, mysql, httpfs)
    - ATTACH å¤–éƒ¨æ•°æ®åº“
    - ä¸ºæ¯ä¸ª RawData åˆ›å»º VIEW
    """
    import duckdb

    session_dir = ensure_session_dir(user_id, thread_id)
    duckdb_path = session_dir / "session.duckdb"

    try:
        # åˆ›å»º/æ‰“å¼€ DuckDB æ–‡ä»¶
        conn = duckdb.connect(str(duckdb_path))

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        extensions_dir.mkdir(parents=True, exist_ok=True)
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        views_created: list[str] = []
        errors: list[str] = []

        # å¦‚æœæ²¡æœ‰æ•°æ®å¯¹è±¡ï¼Œåªåˆ›å»ºç©ºçš„ DuckDB æ–‡ä»¶
        if not request.raw_data_list:
            conn.close()
            return {
                "success": True,
                "message": "Session DuckDB initialized (no raw data)",
                "duckdb_path": str(duckdb_path),
                "views_created": [],
                "errors": [],
            }

        # ä¸ºæ¯ä¸ª RawData åˆ›å»º VIEW
        for raw_data in request.raw_data_list:
            try:
                view_name = raw_data.name  # ä½¿ç”¨ RawData åç§°ä½œä¸º VIEW åç§°

                if raw_data.raw_type == "database_table":
                    # æ•°æ®åº“è¡¨ç±»å‹ï¼šATTACH æ•°æ®åº“å¹¶åˆ›å»º VIEW
                    if raw_data.db_type == "postgresql":
                        conn.execute("INSTALL postgres; LOAD postgres;")
                        conn_str = (
                            f"host={raw_data.host} "
                            f"port={raw_data.port} "
                            f"dbname={raw_data.database} "
                            f"user={raw_data.username} "
                            f"password={raw_data.password}"
                        )
                        attach_name = f"pg_{raw_data.id}"
                        conn.execute(f"ATTACH '{conn_str}' AS {attach_name} (TYPE POSTGRES, READ_ONLY);")

                        # æ„å»ºæºè¡¨å
                        if raw_data.custom_sql:
                            # ä½¿ç”¨è‡ªå®šä¹‰ SQL
                            conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS {raw_data.custom_sql}')
                        else:
                            # ä½¿ç”¨ schema.table
                            schema = raw_data.schema_name or "public"
                            table = raw_data.table_name
                            conn.execute(
                                f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM {attach_name}.{schema}.{table}'
                            )

                    elif raw_data.db_type == "mysql":
                        conn.execute("INSTALL mysql; LOAD mysql;")
                        conn_str = (
                            f"host={raw_data.host} "
                            f"port={raw_data.port} "
                            f"database={raw_data.database} "
                            f"user={raw_data.username} "
                            f"password={raw_data.password}"
                        )
                        attach_name = f"mysql_{raw_data.id}"
                        conn.execute(f"ATTACH '{conn_str}' AS {attach_name} (TYPE MYSQL, READ_ONLY);")

                        if raw_data.custom_sql:
                            conn.execute(f'CREATE OR REPLACE VIEW "{view_name}" AS {raw_data.custom_sql}')
                        else:
                            table = raw_data.table_name
                            conn.execute(
                                f'CREATE OR REPLACE VIEW "{view_name}" AS SELECT * FROM {attach_name}.{table}'
                            )

                    views_created.append(view_name)

                elif raw_data.raw_type == "file":
                    # æ–‡ä»¶ç±»å‹ï¼šé€šè¿‡ S3/httpfs åˆ›å»º VIEW
                    configure_s3_access(conn)

                    s3_url = f"s3://{raw_data.bucket_name}/{raw_data.object_key}"

                    if raw_data.file_type == "csv":
                        conn.execute(
                            f"CREATE OR REPLACE VIEW \"{view_name}\" AS SELECT * FROM read_csv_auto('{s3_url}', header=True)"
                        )
                    elif raw_data.file_type == "parquet":
                        conn.execute(
                            f"CREATE OR REPLACE VIEW \"{view_name}\" AS SELECT * FROM parquet_scan('{s3_url}')"
                        )
                    elif raw_data.file_type == "json":
                        conn.execute(
                            f"CREATE OR REPLACE VIEW \"{view_name}\" AS SELECT * FROM read_json_auto('{s3_url}')"
                        )
                    elif raw_data.file_type == "excel":
                        conn.execute("INSTALL spatial; LOAD spatial;")
                        conn.execute(f"CREATE OR REPLACE VIEW \"{view_name}\" AS SELECT * FROM st_read('{s3_url}')")

                    views_created.append(view_name)

            except Exception as e:
                error_msg = f"Failed to create view for {raw_data.name}: {str(e)}"
                logger.warning(error_msg)
                errors.append(error_msg)

        conn.close()

        logger.info(f"Session DuckDB initialized: user_id={user_id}, thread_id={thread_id}, views={len(views_created)}")

        return {
            "success": True,
            "message": f"Session DuckDB initialized with {len(views_created)} views",
            "duckdb_path": str(duckdb_path),
            "views_created": views_created,
            "errors": errors,
        }

    except Exception as e:
        error_traceback = traceback.format_exc()
        logger.exception(f"Failed to initialize session DuckDB: {e}")
        return {"success": False, "error": f"{str(e)}\n\n{error_traceback}"}


# ==================== é‡ç½®æ“ä½œ ====================


@app.post("/reset/session", summary="Reset session files")
async def reset_session(
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    é‡ç½®æŒ‡å®šä¼šè¯çš„æ–‡ä»¶ã€‚
    åˆ é™¤è¯¥ä¼šè¯ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    """
    return FileService.reset_session(user_id, thread_id)


@app.post("/reset/user", summary="Reset all user sessions")
async def reset_user(
    user_id: str = Query(..., description="User ID"),
):
    """
    é‡ç½®æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯æ–‡ä»¶ã€‚
    åˆ é™¤è¯¥ç”¨æˆ·ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    """
    return FileService.reset_user(user_id)


@app.post("/reset/all", summary="Reset all sandbox data")
async def reset_all():
    """
    é‡ç½®æ‰€æœ‰æ²™ç›’æ•°æ®ã€‚
    åˆ é™¤ sessions ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    ä»…ç”¨äºç®¡ç†ç›®çš„ï¼Œè°¨æ…ä½¿ç”¨ã€‚
    """
    return FileService.reset_all()


# ==================== æ–‡ä»¶ç®¡ç† ====================


@app.get("/files", summary="List files in session directory")
async def list_files(
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åˆ—å‡ºä¼šè¯ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ã€‚
    ç”¨äºæŸ¥çœ‹åˆ†æè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ä¸­é—´æ–‡ä»¶ã€å›¾è¡¨ã€æŠ¥å‘Šç­‰ã€‚
    """
    files = FileService.list_session_files(user_id, thread_id)

    return {
        "success": True,
        "files": files,
        "count": len(files),
    }


@app.post("/upload", summary="Upload a file to the session directory")
async def upload_file(
    file: UploadFile = File(...),
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯ç›®å½•ã€‚
    ç”¨æˆ·æ— éœ€å…³å¿ƒå…·ä½“å­˜å‚¨è·¯å¾„ï¼Œæ–‡ä»¶è‡ªåŠ¨ä¿å­˜åˆ°å¯¹åº”ä¼šè¯ç›®å½•ã€‚
    """
    try:
        content = await file.read()
        file_path = FileService.save_uploaded_file(user_id, thread_id, file.filename, content)
        session_dir = get_session_dir(user_id, thread_id)

        return JSONResponse(
            status_code=200,
            content={
                "success": True,
                "message": f"File '{file.filename}' uploaded successfully.",
                "path": str(file_path.relative_to(session_dir)),
            },
        )
    except Exception as e:
        logger.exception("File upload failed")
        return JSONResponse(status_code=500, content={"success": False, "message": f"Upload failed: {e!s}"})


@app.get("/download/{file_path:path}", summary="Download a file from the session directory")
async def download_file(
    file_path: str,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    ä»ä¼šè¯ç›®å½•ä¸‹è½½æ–‡ä»¶ã€‚
    file_path æ˜¯ç›¸å¯¹äºä¼šè¯ç›®å½•çš„è·¯å¾„ã€‚
    """
    full_path = FileService.get_file_path(user_id, thread_id, file_path)

    if full_path is None:
        raise HTTPException(status_code=404, detail="File not found or access denied")

    return FileResponse(path=str(full_path), media_type="application/octet-stream", filename=Path(file_path).name)


# ==================== ä»£ç æ‰§è¡Œ ====================


@app.post("/execute", summary="Execute a shell command", response_model=ExecuteResponse)
async def execute_command(
    request: ExecuteRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åœ¨ä¼šè¯ç›®å½•ä¸­æ‰§è¡Œ Shell å‘½ä»¤ã€‚
    å‘½ä»¤çš„å·¥ä½œç›®å½•è‡ªåŠ¨è®¾ç½®ä¸ºä¼šè¯ç›®å½•ã€‚
    """
    try:
        session_dir = ensure_session_dir(user_id, thread_id)

        # ä½¿ç”¨ shell=True ä»¥æ”¯æŒç®¡é“å’Œé‡å®šå‘
        process = subprocess.run(
            request.command,
            shell=True,
            capture_output=True,
            text=True,
            cwd=str(session_dir),
            timeout=60,
        )

        return ExecuteResponse(stdout=process.stdout, stderr=process.stderr, exit_code=process.returncode)
    except subprocess.TimeoutExpired:
        return ExecuteResponse(stdout="", stderr="Command execution timeout (60s)", exit_code=124)
    except Exception as e:
        return ExecuteResponse(stdout="", stderr=f"Failed to execute command: {e!s}", exit_code=1)


@app.post("/execute_python", summary="Execute Python code")
async def execute_python(
    request: CodeRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    åœ¨æ²™ç›’ä¸­æ‰§è¡Œ Python ä»£ç ã€‚
    ä»£ç å¯ä»¥è®¿é—® pandasã€numpy ç­‰æ•°æ®åˆ†æåº“ã€‚
    ç”Ÿæˆçš„æ–‡ä»¶ä¼šä¿å­˜åˆ°ä¼šè¯ç›®å½•ã€‚
    """
    session_dir = ensure_session_dir(user_id, thread_id)

    # è·å–æ‰§è¡Œå‰çš„æ–‡ä»¶åˆ—è¡¨
    files_before = set(f["name"] for f in list_files_in_dir(session_dir))

    # æ•è·è¾“å‡º
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()

    # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
    exec_globals = {
        "__builtins__": __builtins__,
        "__name__": "__main__",
        "WORK_DIR": session_dir,
    }

    # åˆ‡æ¢åˆ°ä¼šè¯ç›®å½•
    original_cwd = os.getcwd()
    original_path = sys.path.copy()

    try:
        os.chdir(session_dir)
        sys.path.insert(0, str(session_dir))

        with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):
            exec(request.code, exec_globals)

        # è·å–æ–°åˆ›å»ºçš„æ–‡ä»¶
        files_after = set(f["name"] for f in list_files_in_dir(session_dir))
        files_created = list(files_after - files_before)

        return CodeExecutionResult(
            success=True,
            output=stdout_buffer.getvalue(),
            files_created=files_created,
        )

    except Exception as e:
        error_traceback = traceback.format_exc()
        return CodeExecutionResult(
            success=False,
            output=stdout_buffer.getvalue(),
            error=f"{e!s}\n\n{error_traceback}",
        )
    finally:
        os.chdir(original_cwd)
        sys.path = original_path


@app.post("/execute_sql", summary="Execute SQL query using DuckDB")
async def execute_sql(
    request: SqlRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    ä½¿ç”¨ä¼šè¯çš„ DuckDB æ–‡ä»¶æ‰§è¡Œ SQL æŸ¥è¯¢ã€‚

    æ•°æ®é€šè¿‡ä¼šè¯åˆå§‹åŒ–æ—¶åˆ›å»ºçš„ VIEWs è®¿é—®ï¼ŒAI å¯ä»¥ç›´æ¥æŸ¥è¯¢è¿™äº› VIEWsã€‚
    """
    import duckdb

    session_dir = ensure_session_dir(user_id, thread_id)
    duckdb_path = session_dir / "session.duckdb"

    try:
        # å¦‚æœä¼šè¯ DuckDB æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„
        if not duckdb_path.exists():
            logger.warning(f"Session DuckDB not found, creating empty: {duckdb_path}")

        # æ‰“å¼€ä¼šè¯çš„ DuckDB æ–‡ä»¶
        conn = duckdb.connect(str(duckdb_path))

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        # é…ç½® S3 è®¿é—®ï¼ˆç”¨äºè¯»å–ä¼šè¯ç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶ï¼‰
        configure_s3_access(conn)

        # åˆ‡æ¢å·¥ä½œç›®å½•ä»¥ä¾¿ç›¸å¯¹è·¯å¾„è®¿é—®æœ¬åœ°æ–‡ä»¶
        original_cwd = os.getcwd()
        os.chdir(session_dir)

        try:
            # å…ˆç”¨ EXPLAIN æ£€æŸ¥ SQL è¯­æ³•ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰
            try:
                conn.execute(f"EXPLAIN {request.sql}")
            except Exception as explain_error:
                # è¯­æ³•é”™è¯¯ï¼Œç›´æ¥è¿”å›é”™è¯¯ä¿¡æ¯
                error_msg = str(explain_error)
                logger.warning(f"SQL syntax check failed: {error_msg}")
                return {"success": False, "error": error_msg}

            # è¯­æ³•æ£€æŸ¥é€šè¿‡ï¼Œæ‰§è¡Œå®é™…æŸ¥è¯¢
            result = conn.execute(request.sql)
            columns = [desc[0] for desc in result.description] if result.description else []

            # ä½¿ç”¨ fetchmany é™åˆ¶å†…å­˜ä½¿ç”¨ï¼Œé¿å…å¤§ç»“æœé›†å¯¼è‡´ OOM
            max_rows = request.max_rows
            rows = result.fetchmany(max_rows + 1)  # å¤šå–ä¸€è¡Œç”¨äºæ£€æµ‹æ˜¯å¦æœ‰æ›´å¤šæ•°æ®
            has_more = len(rows) > max_rows
            if has_more:
                rows = rows[:max_rows]  # æˆªæ–­åˆ°é™åˆ¶è¡Œæ•°
                logger.warning(f"SQL result truncated to {max_rows} rows (has more data)")

            # è‡ªåŠ¨ä¿å­˜ç»“æœåˆ° parquet æ–‡ä»¶ï¼ˆä¾›åç»­å·¥å…·ä½¿ç”¨ï¼‰
            result_file = None
            if rows and columns:
                import pandas as pd

                try:
                    df = pd.DataFrame(rows, columns=columns)
                    result_file = generate_unique_filename(session_dir, "sql_result_", ".parquet")
                    df.to_parquet(session_dir / result_file, index=False)
                    logger.info(f"SQL result saved to {result_file}")
                except Exception as e:
                    logger.warning(f"Failed to save SQL result: {e}")

            return {
                "success": True,
                "columns": columns,
                "rows": [list(row) for row in rows],
                "row_count": len(rows),
                "result_file": result_file,  # ç»“æœæ–‡ä»¶è·¯å¾„
                "truncated": has_more,  # æ˜¯å¦è¢«æˆªæ–­
                "max_rows": max_rows,  # æœ€å¤§è¡Œæ•°é™åˆ¶
            }
        finally:
            os.chdir(original_cwd)
            conn.close()

    except Exception as e:
        error_traceback = traceback.format_exc()
        logger.exception("SQL execution failed")
        return {"success": False, "error": f"{e!s}\n\n{error_traceback}"}


# ==================== æ•°æ®åˆ†æ ====================


@app.post("/quick_analysis", summary="Quick data analysis")
async def quick_analysis(
    request: QuickAnalysisRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    å¿«é€Ÿåˆ†ææ•°æ®ï¼Œæ”¯æŒä¸¤ç§æ¨¡å¼ï¼š

    1. åˆ†æä¼šè¯æ–‡ä»¶ï¼šæŒ‡å®š file_name å‚æ•°
    2. åˆ†ææ•°æ®æº VIEWï¼šæŒ‡å®š view_names æˆ–ç•™ç©ºåˆ†ææ‰€æœ‰ VIEW
    """
    import duckdb

    session_dir = get_session_dir(user_id, thread_id)

    # ===== æ¨¡å¼ 1ï¼šåˆ†æä¼šè¯æ–‡ä»¶ =====
    if request.file_name:
        file_path = session_dir / request.file_name

        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„ç©¿è¶Š
        try:
            file_path.resolve().relative_to(session_dir.resolve())
        except ValueError:
            return {"success": False, "error": "Invalid file path: path traversal detected"}

        if not file_path.exists():
            return {"success": False, "error": f"File not found: {request.file_name}"}

        conn = None
        try:
            conn = duckdb.connect(":memory:")
            extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
            conn.execute(f"SET extension_directory='{extensions_dir}';")

            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–æ–¹å¼
            file_ext = file_path.suffix.lower()
            original_cwd = os.getcwd()
            os.chdir(session_dir)

            try:
                if file_ext == ".parquet":
                    conn.execute(f"CREATE OR REPLACE TEMP VIEW data_preview AS SELECT * FROM '{request.file_name}'")
                elif file_ext == ".csv":
                    conn.execute(
                        f"CREATE OR REPLACE TEMP VIEW data_preview AS SELECT * FROM read_csv_auto('{request.file_name}', header=True)"
                    )
                elif file_ext == ".json":
                    conn.execute(
                        f"CREATE OR REPLACE TEMP VIEW data_preview AS SELECT * FROM read_json_auto('{request.file_name}')"
                    )
                else:
                    return {"success": False, "error": f"Unsupported file type: {file_ext}"}

                analysis = analyze_data_with_duckdb(conn, "data_preview")
                analysis["file_name"] = request.file_name

                return {"success": True, "analysis": analysis}
            finally:
                os.chdir(original_cwd)

        except Exception as e:
            logger.exception(f"Failed to analyze file {request.file_name}")
            return {"success": False, "error": str(e)}
        finally:
            if conn:
                conn.close()

    # ===== æ¨¡å¼ 2ï¼šåˆ†ææ•°æ®æº VIEW =====
    duckdb_path = session_dir / "session.duckdb"

    if not duckdb_path.exists():
        return {
            "success": False,
            "error": "Session DuckDB not initialized. Please create a session with data source first.",
        }

    conn = None
    try:
        conn = duckdb.connect(str(duckdb_path), read_only=True)

        # è®¾ç½®æ‰©å±•ç›®å½•
        extensions_dir = SANDBOX_ROOT / "duckdb_extensions"
        conn.execute(f"SET extension_directory='{extensions_dir}';")

        # é…ç½® S3 è®¿é—®ï¼ˆVIEW å¯èƒ½å¼•ç”¨ S3 URLï¼‰
        configure_s3_access(conn)

        # è·å–è¦åˆ†æçš„ VIEW åˆ—è¡¨
        if request.view_names:
            view_names = request.view_names
        else:
            # æŸ¥è¯¢æ‰€æœ‰ VIEW
            views_result = conn.execute(
                "SELECT table_name FROM information_schema.tables WHERE table_type = 'VIEW'"
            ).fetchall()
            view_names = [row[0] for row in views_result]

        if not view_names:
            return {
                "success": True,
                "analysis": {
                    "views": [],
                    "message": "No VIEWs found in session DuckDB",
                },
            }

        # åˆ†ææ¯ä¸ª VIEW
        views_analysis = []
        for view_name in view_names:
            try:
                analysis = analyze_data_with_duckdb(conn, f'"{view_name}"')
                analysis["view_name"] = view_name
                views_analysis.append(analysis)
            except Exception as e:
                logger.warning(f"Failed to analyze view {view_name}: {e}")
                views_analysis.append(
                    {
                        "view_name": view_name,
                        "error": str(e),
                    }
                )

        # å¦‚æœåªæœ‰ä¸€ä¸ª VIEWï¼Œç®€åŒ–è¿”å›ç»“æ„
        if len(views_analysis) == 1:
            result_analysis = views_analysis[0]
        else:
            result_analysis = {
                "view_count": len(views_analysis),
                "views": views_analysis,
            }

        return {"success": True, "analysis": result_analysis}

    except Exception as e:
        logger.exception("Quick analysis failed")
        return {"success": False, "error": str(e)}
    finally:
        if conn:
            conn.close()


# ==================== å›¾è¡¨ç”Ÿæˆ ====================


@app.post("/generate_chart", summary="Generate Plotly chart")
async def generate_chart(
    request: ChartRequest,
    user_id: str = Query(..., description="User ID"),
    thread_id: str = Query(..., description="Thread/Session ID"),
):
    """
    æ‰§è¡Œ Python ä»£ç ç”Ÿæˆ Plotly å›¾è¡¨ã€‚

    ä»£ç åº”è¯¥ä½¿ç”¨ plotly åº“åˆ›å»ºå›¾è¡¨ï¼Œå¹¶å°† figure å¯¹è±¡èµ‹å€¼ç»™ `fig` å˜é‡ã€‚
    ç¤ºä¾‹ä»£ç ï¼š
    ```python
    import plotly.express as px
    import pandas as pd

    df = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]})
    fig = px.bar(df, x='x', y='y', title='ç¤ºä¾‹å›¾è¡¨')
    ```
    """
    session_dir = ensure_session_dir(user_id, thread_id)

    # æ•è·è¾“å‡º
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()

    # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
    exec_globals = {
        "__builtins__": __builtins__,
        "__name__": "__main__",
        "WORK_DIR": session_dir,
    }

    # åˆ‡æ¢åˆ°ä¼šè¯ç›®å½•
    original_cwd = os.getcwd()
    original_path = sys.path.copy()

    try:
        os.chdir(session_dir)
        sys.path.insert(0, str(session_dir))

        with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):
            exec(request.code, exec_globals)

        # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº† fig å˜é‡
        fig = exec_globals.get("fig")
        if fig is None:
            return {
                "success": False,
                "error": "ä»£ç æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªæ‰¾åˆ° 'fig' å˜é‡ã€‚è¯·ç¡®ä¿ä»£ç åˆ›å»ºäº†åä¸º 'fig' çš„ Plotly figure å¯¹è±¡ã€‚",
                "output": stdout_buffer.getvalue(),
            }

        # åŒæ—¶ä¿å­˜ä¸º JSON ä»¥ä¾¿å‰ç«¯æ¸²æŸ“
        chart_json = fig.to_json()

        return {
            "success": True,
            "chart_json": chart_json,
            "output": stdout_buffer.getvalue(),
        }

    except Exception as e:
        error_traceback = traceback.format_exc()
        return {
            "success": False,
            "output": stdout_buffer.getvalue(),
            "error": f"{e!s}\n\n{error_traceback}",
        }
    finally:
        os.chdir(original_cwd)
        sys.path = original_path
