#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šCSV ä¸Šä¼ åˆ° MinIO + DuckDB ç›´æ¥ä» S3 åˆ†æ

æµ‹è¯•å†…å®¹ï¼š
1. MinIO æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½
2. DuckDB ç›´æ¥ä» S3 åˆ†æï¼ˆæœ¬åœ°è°ƒç”¨ï¼‰
3. Sandbox API quick_analysis æ¥å£ï¼ˆéœ€è¦ sandbox_runtime è¿è¡Œï¼‰

è¿è¡Œæ–¹å¼ï¼š
    cd /data/zhanghuaao/project/fast-data-agent
    source .venv/bin/activate
    python scripts/test_minio_duckdb.py
"""

import asyncio
import io
from pathlib import Path

import duckdb
import httpx
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.minio import minio_client
from app.core.config import settings


def create_sample_csv() -> tuple[bytes, str]:
    """åˆ›å»ºç¤ºä¾‹ CSV æ•°æ®"""
    data = {
        "id": range(1, 101),
        "name": [f"ç”¨æˆ·_{i}" for i in range(1, 101)],
        "age": [20 + (i % 50) for i in range(100)],
        "salary": [5000 + (i * 100) + (i % 7) * 500 for i in range(100)],
        "department": ["æŠ€æœ¯", "é”€å”®", "è¿è¥", "è´¢åŠ¡", "äººäº‹"] * 20,
        "city": ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"] * 20,
        "score": [60 + (i % 40) + (i % 3) * 5 for i in range(100)],
    }
    df = pd.DataFrame(data)
    
    # æ·»åŠ ä¸€äº›ç©ºå€¼ç”¨äºæµ‹è¯•
    df.loc[5, "age"] = None
    df.loc[15, "salary"] = None
    df.loc[25, "score"] = None
    
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)
    return csv_buffer.getvalue().encode("utf-8"), "test_data.csv"


async def test_upload_to_minio(csv_data: bytes, filename: str) -> str:
    """æµ‹è¯•ä¸Šä¼  CSV åˆ° MinIO"""
    print("\n" + "=" * 60)
    print("ğŸ“¤ æµ‹è¯• 1: ä¸Šä¼  CSV åˆ° MinIO")
    print("=" * 60)
    
    object_key = f"test/{filename}"
    
    await minio_client.upload_file(
        object_name=object_key,
        data=csv_data,
        length=len(csv_data),
        content_type="text/csv",
    )
    
    print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
    print(f"   - Bucket: {settings.MINIO_BUCKET}")
    print(f"   - Object Key: {object_key}")
    print(f"   - æ–‡ä»¶å¤§å°: {len(csv_data):,} bytes")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    exists = await minio_client.file_exists(object_key)
    print(f"   - æ–‡ä»¶å­˜åœ¨æ£€æŸ¥: {'âœ… å­˜åœ¨' if exists else 'âŒ ä¸å­˜åœ¨'}")
    
    return object_key


def setup_duckdb_s3() -> duckdb.DuckDBPyConnection:
    """é…ç½® DuckDB ä»¥è®¿é—® MinIO (S3 å…¼å®¹)"""
    conn = duckdb.connect(":memory:")
    
    # å®‰è£…å¹¶åŠ è½½ httpfs æ‰©å±•
    conn.execute("INSTALL httpfs;")
    conn.execute("LOAD httpfs;")
    
    # é…ç½® S3 è¿æ¥å‚æ•° (MinIO å…¼å®¹)
    conn.execute(f"SET s3_endpoint='{settings.MINIO_ENDPOINT}';")
    conn.execute(f"SET s3_access_key_id='{settings.MINIO_ACCESS_KEY}';")
    conn.execute(f"SET s3_secret_access_key='{settings.MINIO_SECRET_KEY}';")
    conn.execute("SET s3_url_style='path';")  # MinIO ä½¿ç”¨ path style
    conn.execute(f"SET s3_use_ssl={'true' if settings.MINIO_SECURE else 'false'};")
    
    return conn


def test_duckdb_direct_s3_analysis(object_key: str) -> dict:
    """ä½¿ç”¨ DuckDB ç›´æ¥ä» S3/MinIO åˆ†ææ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯• 2: DuckDB ç›´æ¥ä» S3/MinIO åˆ†æ")
    print("=" * 60)
    
    # æ„å»º S3 URL
    s3_url = f"s3://{settings.MINIO_BUCKET}/{object_key}"
    print(f"\nğŸ“ S3 URL: {s3_url}")
    
    # é…ç½® DuckDB S3 è¿æ¥
    conn = setup_duckdb_s3()
    print("âœ… DuckDB S3 è¿æ¥é…ç½®å®Œæˆ")
    
    try:
        # ç›´æ¥ä» S3 è¯»å– CSV æ–‡ä»¶
        conn.execute(f"""
            CREATE TABLE data AS 
            SELECT * FROM read_csv_auto('{s3_url}', header=True)
        """)
        print("âœ… ç›´æ¥ä» S3 è¯»å–æ•°æ®æˆåŠŸ")
        
        # 1. åŸºæœ¬ä¿¡æ¯
        print("\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        row_count = conn.execute("SELECT COUNT(*) FROM data").fetchone()[0]
        columns_info = conn.execute("PRAGMA table_info('data')").fetchall()
        
        print(f"   - æ€»è¡Œæ•°: {row_count:,}")
        print(f"   - æ€»åˆ—æ•°: {len(columns_info)}")
        print(f"   - åˆ—å: {[col[1] for col in columns_info]}")
        
        # 2. æ•°æ®ç±»å‹
        print("\nğŸ“ åˆ—ä¿¡æ¯:")
        for col in columns_info:
            col_name, col_type = col[1], col[2]
            null_count = conn.execute(
                f'SELECT COUNT(*) FROM data WHERE "{col_name}" IS NULL'
            ).fetchone()[0]
            print(f"   - {col_name}: {col_type} (ç©ºå€¼: {null_count})")
        
        # 3. æ•°å€¼åˆ—ç»Ÿè®¡
        print("\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡:")
        numeric_cols = ["age", "salary", "score"]
        
        for col in numeric_cols:
            stats = conn.execute(f"""
                SELECT 
                    AVG("{col}") as mean,
                    STDDEV_POP("{col}") as std,
                    MIN("{col}") as min,
                    MAX("{col}") as max,
                    MEDIAN("{col}") as median,
                    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY "{col}") as q1,
                    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY "{col}") as q3
                FROM data
                WHERE "{col}" IS NOT NULL
            """).fetchone()
            
            print(f"\n   {col}:")
            print(f"      å‡å€¼: {stats[0]:.2f}")
            print(f"      æ ‡å‡†å·®: {stats[1]:.2f}")
            print(f"      æœ€å°å€¼: {stats[2]:.2f}")
            print(f"      æœ€å¤§å€¼: {stats[3]:.2f}")
            print(f"      ä¸­ä½æ•°: {stats[4]:.2f}")
            print(f"      Q1(25%): {stats[5]:.2f}")
            print(f"      Q3(75%): {stats[6]:.2f}")
        
        # 4. åˆ†ç±»ç»Ÿè®¡
        print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        
        # éƒ¨é—¨åˆ†å¸ƒ
        dept_stats = conn.execute("""
            SELECT department, COUNT(*) as count, AVG(salary) as avg_salary
            FROM data
            GROUP BY department
            ORDER BY count DESC
        """).fetchall()
        
        print("\n   éƒ¨é—¨åˆ†å¸ƒ:")
        for dept, count, avg_salary in dept_stats:
            print(f"      {dept}: {count} äºº, å¹³å‡è–ªèµ„ {avg_salary:,.0f}")
        
        # åŸå¸‚åˆ†å¸ƒ
        city_stats = conn.execute("""
            SELECT city, COUNT(*) as count, AVG(age) as avg_age
            FROM data
            GROUP BY city
            ORDER BY count DESC
        """).fetchall()
        
        print("\n   åŸå¸‚åˆ†å¸ƒ:")
        for city, count, avg_age in city_stats:
            print(f"      {city}: {count} äºº, å¹³å‡å¹´é¾„ {avg_age:.1f}")
        
        # 5. ç›¸å…³æ€§åˆ†æ
        print("\nğŸ”— ç›¸å…³æ€§åˆ†æ (age vs salary):")
        corr = conn.execute("""
            SELECT CORR(age, salary) as correlation
            FROM data
            WHERE age IS NOT NULL AND salary IS NOT NULL
        """).fetchone()[0]
        print(f"   ç›¸å…³ç³»æ•°: {corr:.4f}")
        
        # 6. æ•°æ®é¢„è§ˆ
        print("\nğŸ‘€ æ•°æ®é¢„è§ˆ (å‰ 5 è¡Œ):")
        preview = conn.execute("SELECT * FROM data LIMIT 5").fetchall()
        columns = [col[1] for col in columns_info]
        
        # æ‰“å°è¡¨å¤´
        header = " | ".join(f"{col:>10}" for col in columns)
        print(f"   {header}")
        print(f"   {'-' * len(header)}")
        
        # æ‰“å°æ•°æ®
        for row in preview:
            row_str = " | ".join(f"{str(val):>10}" for val in row)
            print(f"   {row_str}")
        
        conn.close()
        
        return {
            "row_count": row_count,
            "column_count": len(columns_info),
            "columns": columns,
        }
        
    except Exception as e:
        conn.close()
        raise e


async def test_sandbox_quick_analysis(object_key: str) -> dict | None:
    """æµ‹è¯• Sandbox Runtime çš„ quick_analysis æ¥å£"""
    print("\n" + "=" * 60)
    print("ğŸ”§ æµ‹è¯• 3: Sandbox Runtime quick_analysis æ¥å£")
    print("=" * 60)
    
    sandbox_url = settings.SANDBOX_URL
    print(f"\nğŸ“ Sandbox URL: {sandbox_url}")
    
    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "data_source": {
            "source_type": "file",
            "file_type": "csv",
            "object_key": object_key,
            "bucket_name": settings.MINIO_BUCKET,
        }
    }
    
    print(f"ğŸ“¤ è¯·æ±‚æ•°æ®: {request_data}")
    
    try:
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                f"{sandbox_url}/quick_analysis",
                params={"user_id": 1, "thread_id": 1},
                json=request_data,
            )
            
            result = response.json()
            
            if result.get("success"):
                print("âœ… Sandbox API è°ƒç”¨æˆåŠŸ")
                analysis = result.get("analysis", {})
                print(f"\nğŸ“Š åˆ†æç»“æœ:")
                print(f"   - æ•°æ®æºç±»å‹: {analysis.get('source_type')}")
                print(f"   - æ–‡ä»¶å: {analysis.get('file_name')}")
                print(f"   - è¡Œæ•°: {analysis.get('row_count')}")
                print(f"   - åˆ—æ•°: {analysis.get('column_count')}")
                
                if analysis.get("columns"):
                    print(f"\n   åˆ—ä¿¡æ¯:")
                    for col in analysis["columns"][:5]:  # åªæ˜¾ç¤ºå‰ 5 åˆ—
                        print(f"      - {col['name']}: {col['dtype']} (ç©ºå€¼: {col['null_count']})")
                        if col.get("stats"):
                            stats = col["stats"]
                            print(f"        å‡å€¼: {stats.get('mean', 'N/A'):.2f}, "
                                  f"æ ‡å‡†å·®: {stats.get('std', 'N/A'):.2f}")
                
                return result
            else:
                print(f"âŒ Sandbox API è°ƒç”¨å¤±è´¥: {result.get('error')}")
                return None
                
    except httpx.ConnectError:
        print(f"âš ï¸ æ— æ³•è¿æ¥åˆ° Sandbox Runtime ({sandbox_url})")
        print("   è¯·ç¡®ä¿ sandbox_runtime æ­£åœ¨è¿è¡Œ")
        return None
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None


async def test_cleanup(object_key: str):
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    print("\n" + "=" * 60)
    print("ğŸ§¹ æµ‹è¯• 4: æ¸…ç†æµ‹è¯•æ•°æ®")
    print("=" * 60)
    
    await minio_client.delete_file(object_key)
    
    exists = await minio_client.file_exists(object_key)
    print(f"âœ… æ–‡ä»¶åˆ é™¤æˆåŠŸ")
    print(f"   - æ–‡ä»¶å­˜åœ¨æ£€æŸ¥: {'âŒ ä»å­˜åœ¨' if exists else 'âœ… å·²åˆ é™¤'}")


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€" * 20)
    print("  CSV ä¸Šä¼  MinIO + DuckDB ç»Ÿè®¡åˆ†æ æµ‹è¯•")
    print("ğŸš€" * 20)
    
    print(f"\nğŸ“ MinIO é…ç½®:")
    print(f"   - Endpoint: {settings.MINIO_ENDPOINT}")
    print(f"   - Bucket: {settings.MINIO_BUCKET}")
    print(f"   - Secure: {settings.MINIO_SECURE}")
    
    try:
        # Step 1: åˆ›å»ºç¤ºä¾‹æ•°æ®
        csv_data, filename = create_sample_csv()
        print(f"\nâœ… ç¤ºä¾‹ CSV æ•°æ®å·²åˆ›å»º ({len(csv_data):,} bytes)")
        
        # Step 2: ä¸Šä¼ åˆ° MinIO
        object_key = await test_upload_to_minio(csv_data, filename)
        
        # Step 3: DuckDB ç›´æ¥ä» S3/MinIO åˆ†æï¼ˆæœ¬åœ°è°ƒç”¨ï¼‰
        analysis_result = test_duckdb_direct_s3_analysis(object_key)
        
        # Step 4: æµ‹è¯• Sandbox Runtime APIï¼ˆå¦‚æœå¯ç”¨ï¼‰
        sandbox_result = await test_sandbox_quick_analysis(object_key)
        
        # Step 5: æ¸…ç†
        await test_cleanup(object_key)
        
        # æ€»ç»“
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 60)
        print(f"\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
        print(f"   - æ•°æ®è¡Œæ•°: {analysis_result['row_count']}")
        print(f"   - æ•°æ®åˆ—æ•°: {analysis_result['column_count']}")
        print(f"   - åˆ—å: {', '.join(analysis_result['columns'])}")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)

