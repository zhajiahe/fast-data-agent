#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šå¤šç§æ–‡ä»¶æ ¼å¼ä¸Šä¼ åˆ° MinIO + DuckDB ç›´æ¥ä» S3 åˆ†æ

æµ‹è¯•å†…å®¹ï¼š
1. æ–‡ä»¶ç±»å‹: CSV, JSON, Parquet, Excel
2. æ•°æ®åº“ç±»å‹: SQLite
3. DuckDB ç›´æ¥ä» S3 åˆ†æ
4. Sandbox API quick_analysis æ¥å£

è¿è¡Œæ–¹å¼ï¼š
    cd /data/zhanghuaao/project/fast-data-agent
    source .venv/bin/activate
    python scripts/test_minio_duckdb.py
"""

import asyncio
import io
import sqlite3
import tempfile
from pathlib import Path

import duckdb
import httpx
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.minio import minio_client
from app.core.config import settings


def create_sample_dataframe() -> pd.DataFrame:
    """åˆ›å»ºç¤ºä¾‹æ•°æ® DataFrame"""
    data = {
        "id": range(1, 101),
        "name": [f"ç”¨æˆ·_{i}" for i in range(1, 101)],
        "age": [20 + (i % 50) for i in range(100)],
        "salary": [5000 + (i * 100) + (i % 7) * 500 for i in range(100)],
        "department": ["æŠ€æœ¯", "é”€å”®", "è¿è¥", "è´¢åŠ¡", "äººäº‹"] * 20,
        "city": ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"] * 20,
        "score": [60 + (i % 40) + (i % 3) * 5 for i in range(100)],
    }
    df = pd.DataFrame(data)
    
    # æ·»åŠ ä¸€äº›ç©ºå€¼ç”¨äºæµ‹è¯•
    df.loc[5, "age"] = None
    df.loc[15, "salary"] = None
    df.loc[25, "score"] = None
    
    return df


def create_sample_csv() -> tuple[bytes, str]:
    """åˆ›å»ºç¤ºä¾‹ CSV æ•°æ®"""
    df = create_sample_dataframe()
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)
    return csv_buffer.getvalue().encode("utf-8"), "test_data.csv"


def create_sample_json() -> tuple[bytes, str]:
    """åˆ›å»ºç¤ºä¾‹ JSON æ•°æ®"""
    df = create_sample_dataframe()
    json_str = df.to_json(orient="records", force_ascii=False)
    return json_str.encode("utf-8"), "test_data.json"


def create_sample_parquet() -> tuple[bytes, str]:
    """åˆ›å»ºç¤ºä¾‹ Parquet æ•°æ®"""
    df = create_sample_dataframe()
    buffer = io.BytesIO()
    df.to_parquet(buffer, index=False, engine="pyarrow")
    return buffer.getvalue(), "test_data.parquet"


def create_sample_excel() -> tuple[bytes, str]:
    """åˆ›å»ºç¤ºä¾‹ Excel æ•°æ®"""
    df = create_sample_dataframe()
    buffer = io.BytesIO()
    df.to_excel(buffer, index=False, engine="openpyxl")
    return buffer.getvalue(), "test_data.xlsx"


def create_sample_sqlite() -> str:
    """åˆ›å»ºç¤ºä¾‹ SQLite æ•°æ®åº“å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    df = create_sample_dataframe()
    
    # åˆ›å»ºä¸´æ—¶ SQLite æ–‡ä»¶
    temp_file = tempfile.NamedTemporaryFile(suffix=".db", delete=False)
    db_path = temp_file.name
    temp_file.close()
    
    # å†™å…¥æ•°æ®
    conn = sqlite3.connect(db_path)
    df.to_sql("users", conn, index=False, if_exists="replace")
    conn.close()
    
    return db_path


async def test_upload_to_minio(csv_data: bytes, filename: str) -> str:
    """æµ‹è¯•ä¸Šä¼  CSV åˆ° MinIO"""
    print("\n" + "=" * 60)
    print("ğŸ“¤ æµ‹è¯• 1: ä¸Šä¼  CSV åˆ° MinIO")
    print("=" * 60)
    
    object_key = f"test/{filename}"
    
    await minio_client.upload_file(
        object_name=object_key,
        data=csv_data,
        length=len(csv_data),
        content_type="text/csv",
    )
    
    print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
    print(f"   - Bucket: {settings.MINIO_BUCKET}")
    print(f"   - Object Key: {object_key}")
    print(f"   - æ–‡ä»¶å¤§å°: {len(csv_data):,} bytes")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    exists = await minio_client.file_exists(object_key)
    print(f"   - æ–‡ä»¶å­˜åœ¨æ£€æŸ¥: {'âœ… å­˜åœ¨' if exists else 'âŒ ä¸å­˜åœ¨'}")
    
    return object_key


def setup_duckdb_s3() -> duckdb.DuckDBPyConnection:
    """é…ç½® DuckDB ä»¥è®¿é—® MinIO (S3 å…¼å®¹)"""
    conn = duckdb.connect(":memory:")
    
    # å®‰è£…å¹¶åŠ è½½ httpfs æ‰©å±•
    conn.execute("INSTALL httpfs;")
    conn.execute("LOAD httpfs;")
    
    # é…ç½® S3 è¿æ¥å‚æ•° (MinIO å…¼å®¹)
    conn.execute(f"SET s3_endpoint='{settings.MINIO_ENDPOINT}';")
    conn.execute(f"SET s3_access_key_id='{settings.MINIO_ACCESS_KEY}';")
    conn.execute(f"SET s3_secret_access_key='{settings.MINIO_SECRET_KEY}';")
    conn.execute("SET s3_url_style='path';")  # MinIO ä½¿ç”¨ path style
    conn.execute(f"SET s3_use_ssl={'true' if settings.MINIO_SECURE else 'false'};")
    
    return conn


def test_duckdb_direct_s3_analysis(object_key: str) -> dict:
    """ä½¿ç”¨ DuckDB ç›´æ¥ä» S3/MinIO åˆ†ææ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯• 2: DuckDB ç›´æ¥ä» S3/MinIO åˆ†æ")
    print("=" * 60)
    
    # æ„å»º S3 URL
    s3_url = f"s3://{settings.MINIO_BUCKET}/{object_key}"
    print(f"\nğŸ“ S3 URL: {s3_url}")
    
    # é…ç½® DuckDB S3 è¿æ¥
    conn = setup_duckdb_s3()
    print("âœ… DuckDB S3 è¿æ¥é…ç½®å®Œæˆ")
    
    try:
        # ç›´æ¥ä» S3 è¯»å– CSV æ–‡ä»¶
        conn.execute(f"""
            CREATE TABLE data AS 
            SELECT * FROM read_csv_auto('{s3_url}', header=True)
        """)
        print("âœ… ç›´æ¥ä» S3 è¯»å–æ•°æ®æˆåŠŸ")
        
        # 1. åŸºæœ¬ä¿¡æ¯
        print("\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        row_count = conn.execute("SELECT COUNT(*) FROM data").fetchone()[0]
        columns_info = conn.execute("PRAGMA table_info('data')").fetchall()
        
        print(f"   - æ€»è¡Œæ•°: {row_count:,}")
        print(f"   - æ€»åˆ—æ•°: {len(columns_info)}")
        print(f"   - åˆ—å: {[col[1] for col in columns_info]}")
        
        # 2. æ•°æ®ç±»å‹
        print("\nğŸ“ åˆ—ä¿¡æ¯:")
        for col in columns_info:
            col_name, col_type = col[1], col[2]
            null_count = conn.execute(
                f'SELECT COUNT(*) FROM data WHERE "{col_name}" IS NULL'
            ).fetchone()[0]
            print(f"   - {col_name}: {col_type} (ç©ºå€¼: {null_count})")
        
        # 3. æ•°å€¼åˆ—ç»Ÿè®¡
        print("\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡:")
        numeric_cols = ["age", "salary", "score"]
        
        for col in numeric_cols:
            stats = conn.execute(f"""
                SELECT 
                    AVG("{col}") as mean,
                    STDDEV_POP("{col}") as std,
                    MIN("{col}") as min,
                    MAX("{col}") as max,
                    MEDIAN("{col}") as median,
                    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY "{col}") as q1,
                    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY "{col}") as q3
                FROM data
                WHERE "{col}" IS NOT NULL
            """).fetchone()
            
            print(f"\n   {col}:")
            print(f"      å‡å€¼: {stats[0]:.2f}")
            print(f"      æ ‡å‡†å·®: {stats[1]:.2f}")
            print(f"      æœ€å°å€¼: {stats[2]:.2f}")
            print(f"      æœ€å¤§å€¼: {stats[3]:.2f}")
            print(f"      ä¸­ä½æ•°: {stats[4]:.2f}")
            print(f"      Q1(25%): {stats[5]:.2f}")
            print(f"      Q3(75%): {stats[6]:.2f}")
        
        # 4. åˆ†ç±»ç»Ÿè®¡
        print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        
        # éƒ¨é—¨åˆ†å¸ƒ
        dept_stats = conn.execute("""
            SELECT department, COUNT(*) as count, AVG(salary) as avg_salary
            FROM data
            GROUP BY department
            ORDER BY count DESC
        """).fetchall()
        
        print("\n   éƒ¨é—¨åˆ†å¸ƒ:")
        for dept, count, avg_salary in dept_stats:
            print(f"      {dept}: {count} äºº, å¹³å‡è–ªèµ„ {avg_salary:,.0f}")
        
        # åŸå¸‚åˆ†å¸ƒ
        city_stats = conn.execute("""
            SELECT city, COUNT(*) as count, AVG(age) as avg_age
            FROM data
            GROUP BY city
            ORDER BY count DESC
        """).fetchall()
        
        print("\n   åŸå¸‚åˆ†å¸ƒ:")
        for city, count, avg_age in city_stats:
            print(f"      {city}: {count} äºº, å¹³å‡å¹´é¾„ {avg_age:.1f}")
        
        # 5. ç›¸å…³æ€§åˆ†æ
        print("\nğŸ”— ç›¸å…³æ€§åˆ†æ (age vs salary):")
        corr = conn.execute("""
            SELECT CORR(age, salary) as correlation
            FROM data
            WHERE age IS NOT NULL AND salary IS NOT NULL
        """).fetchone()[0]
        print(f"   ç›¸å…³ç³»æ•°: {corr:.4f}")
        
        # 6. æ•°æ®é¢„è§ˆ
        print("\nğŸ‘€ æ•°æ®é¢„è§ˆ (å‰ 5 è¡Œ):")
        preview = conn.execute("SELECT * FROM data LIMIT 5").fetchall()
        columns = [col[1] for col in columns_info]
        
        # æ‰“å°è¡¨å¤´
        header = " | ".join(f"{col:>10}" for col in columns)
        print(f"   {header}")
        print(f"   {'-' * len(header)}")
        
        # æ‰“å°æ•°æ®
        for row in preview:
            row_str = " | ".join(f"{str(val):>10}" for val in row)
            print(f"   {row_str}")
        
        conn.close()
        
        return {
            "row_count": row_count,
            "column_count": len(columns_info),
            "columns": columns,
        }
        
    except Exception as e:
        conn.close()
        raise e


async def test_sandbox_quick_analysis(object_key: str, file_type: str = "csv") -> dict | None:
    """æµ‹è¯• Sandbox Runtime çš„ quick_analysis æ¥å£ï¼ˆæ–‡ä»¶ç±»å‹ï¼‰"""
    print(f"\nğŸ“ Sandbox URL: {settings.SANDBOX_URL}")
    
    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "data_source": {
            "source_type": "file",
            "file_type": file_type,
            "object_key": object_key,
            "bucket_name": settings.MINIO_BUCKET,
        }
    }
    
    print(f"ğŸ“¤ è¯·æ±‚æ•°æ®: {request_data}")
    
    try:
        async with httpx.AsyncClient(timeout=120) as client:
            response = await client.post(
                f"{settings.SANDBOX_URL}/quick_analysis",
                params={"user_id": 1, "thread_id": 1},
                json=request_data,
            )
            
            result = response.json()
            
            if result.get("success"):
                print("âœ… Sandbox API è°ƒç”¨æˆåŠŸ")
                analysis = result.get("analysis", {})
                print(f"   - è¡Œæ•°: {analysis.get('row_count')}, åˆ—æ•°: {analysis.get('column_count')}")
                return result
            else:
                print(f"âŒ Sandbox API è°ƒç”¨å¤±è´¥: {result.get('error')}")
                return None
                
    except httpx.ConnectError:
        print(f"âš ï¸ æ— æ³•è¿æ¥åˆ° Sandbox Runtime")
        return None
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None


async def create_sqlite_in_sandbox() -> str | None:
    """åœ¨ Sandbox å®¹å™¨å†…åˆ›å»º SQLite æ•°æ®åº“"""
    # åˆ›å»º DataFrame æ•°æ®çš„ Python ä»£ç 
    code = '''
import sqlite3
import pandas as pd

# åˆ›å»ºç¤ºä¾‹æ•°æ®
data = {
    "id": list(range(1, 101)),
    "name": [f"ç”¨æˆ·_{i}" for i in range(1, 101)],
    "age": [20 + (i % 50) for i in range(100)],
    "salary": [5000 + (i * 100) + (i % 7) * 500 for i in range(100)],
    "department": ["æŠ€æœ¯", "é”€å”®", "è¿è¥", "è´¢åŠ¡", "äººäº‹"] * 20,
    "city": ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"] * 20,
}
df = pd.DataFrame(data)

# åˆ›å»º SQLite æ•°æ®åº“
db_path = str(WORK_DIR / "test.db")
conn = sqlite3.connect(db_path)
df.to_sql("users", conn, index=False, if_exists="replace")
conn.close()

print(f"SQLite æ•°æ®åº“å·²åˆ›å»º: {db_path}")
'''
    
    try:
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                f"{settings.SANDBOX_URL}/execute_python",
                params={"user_id": 1, "thread_id": 1},
                json={"code": code},
            )
            result = response.json()
            if result.get("success"):
                # è¿”å›å®¹å™¨å†…çš„æ•°æ®åº“è·¯å¾„
                return "/app/sessions/1/1/test.db"
            else:
                print(f"âŒ åˆ›å»º SQLite å¤±è´¥: {result.get('error')}")
                return None
    except Exception as e:
        print(f"âŒ åˆ›å»º SQLite å¤±è´¥: {e}")
        return None


async def test_sandbox_sqlite_analysis(db_path: str) -> dict | None:
    """æµ‹è¯• Sandbox Runtime çš„ quick_analysis æ¥å£ï¼ˆSQLiteï¼‰"""
    print(f"\nğŸ“ Sandbox URL: {settings.SANDBOX_URL}")
    print(f"ğŸ“ SQLite è·¯å¾„ (å®¹å™¨å†…): {db_path}")
    
    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "data_source": {
            "source_type": "database",
            "db_type": "sqlite",
            "database": db_path,
        }
    }
    
    print(f"ğŸ“¤ è¯·æ±‚æ•°æ®: {request_data}")
    
    try:
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                f"{settings.SANDBOX_URL}/quick_analysis",
                params={"user_id": 1, "thread_id": 1},
                json=request_data,
            )
            
            result = response.json()
            
            if result.get("success"):
                print("âœ… Sandbox API è°ƒç”¨æˆåŠŸ")
                analysis = result.get("analysis", {})
                print(f"   - æ•°æ®åº“ç±»å‹: {analysis.get('db_type')}")
                print(f"   - è¡¨æ•°é‡: {analysis.get('table_count')}")
                if analysis.get("tables"):
                    for table in analysis["tables"]:
                        print(f"   - è¡¨ {table['table_name']}: {table.get('row_count', 'N/A')} è¡Œ")
                return result
            else:
                print(f"âŒ Sandbox API è°ƒç”¨å¤±è´¥: {result.get('error')}")
                return None
                
    except httpx.ConnectError:
        print(f"âš ï¸ æ— æ³•è¿æ¥åˆ° Sandbox Runtime")
        return None
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None


async def test_file_format(file_type: str, create_func, mime_type: str):
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶æ ¼å¼"""
    print("\n" + "=" * 60)
    print(f"ğŸ“¦ æµ‹è¯•æ–‡ä»¶æ ¼å¼: {file_type.upper()}")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ–‡ä»¶
        file_data, filename = create_func()
        print(f"âœ… åˆ›å»º {file_type} æ–‡ä»¶: {len(file_data):,} bytes")
        
        # ä¸Šä¼ åˆ° MinIO
        object_key = f"test/{filename}"
        await minio_client.upload_file(
            object_name=object_key,
            data=file_data,
            length=len(file_data),
            content_type=mime_type,
        )
        print(f"âœ… ä¸Šä¼ æˆåŠŸ: {object_key}")
        
        # è°ƒç”¨ Sandbox API
        result = await test_sandbox_quick_analysis(object_key, file_type)
        
        # æ¸…ç†
        await minio_client.delete_file(object_key)
        print(f"âœ… æ¸…ç†å®Œæˆ")
        
        return result is not None
        
    except Exception as e:
        print(f"âŒ æµ‹è¯• {file_type} å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_sqlite():
    """æµ‹è¯• SQLite æ•°æ®åº“"""
    print("\n" + "=" * 60)
    print("ğŸ“¦ æµ‹è¯•æ•°æ®åº“ç±»å‹: SQLite")
    print("=" * 60)
    
    try:
        # åœ¨ Sandbox å®¹å™¨å†…åˆ›å»º SQLite æ•°æ®åº“
        print("ğŸ“ åœ¨ Sandbox å®¹å™¨å†…åˆ›å»º SQLite æ•°æ®åº“...")
        db_path = await create_sqlite_in_sandbox()
        
        if not db_path:
            print("âŒ æ— æ³•åˆ›å»º SQLite æ•°æ®åº“")
            return False
        
        print(f"âœ… SQLite æ•°æ®åº“å·²åˆ›å»º: {db_path}")
        
        # è°ƒç”¨ Sandbox API åˆ†æ
        result = await test_sandbox_sqlite_analysis(db_path)
        
        return result is not None
        
    except Exception as e:
        print(f"âŒ æµ‹è¯• SQLite å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_cleanup(object_key: str):
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    print("\n" + "=" * 60)
    print("ğŸ§¹ æµ‹è¯• 4: æ¸…ç†æµ‹è¯•æ•°æ®")
    print("=" * 60)
    
    await minio_client.delete_file(object_key)
    
    exists = await minio_client.file_exists(object_key)
    print(f"âœ… æ–‡ä»¶åˆ é™¤æˆåŠŸ")
    print(f"   - æ–‡ä»¶å­˜åœ¨æ£€æŸ¥: {'âŒ ä»å­˜åœ¨' if exists else 'âœ… å·²åˆ é™¤'}")


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€" * 20)
    print("  å¤šæ ¼å¼æ–‡ä»¶ + æ•°æ®åº“ æµ‹è¯•")
    print("ğŸš€" * 20)
    
    print(f"\nğŸ“ MinIO é…ç½®:")
    print(f"   - Endpoint: {settings.MINIO_ENDPOINT}")
    print(f"   - Bucket: {settings.MINIO_BUCKET}")
    print(f"   - Secure: {settings.MINIO_SECURE}")
    print(f"   - Sandbox URL: {settings.SANDBOX_URL}")
    
    results = {}
    
    # æµ‹è¯•å„ç§æ–‡ä»¶æ ¼å¼
    file_tests = [
        ("csv", create_sample_csv, "text/csv"),
        ("json", create_sample_json, "application/json"),
        ("parquet", create_sample_parquet, "application/octet-stream"),
        ("excel", create_sample_excel, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"),
    ]
    
    for file_type, create_func, mime_type in file_tests:
        results[file_type] = await test_file_format(file_type, create_func, mime_type)
    
    # æµ‹è¯• SQLite
    results["sqlite"] = await test_sqlite()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    all_passed = True
    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"   {test_name.upper():>10}: {status}")
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    print("=" * 60)
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)

